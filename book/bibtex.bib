% This file was created with JabRef 2.7.2.
% Encoding: UTF-8

@ARTICLE{Aitken1935,
  author = {Aitken, A. C.},
  title = {{On least squares and linear combination of observations}},
  journal = {Proceedings of the Royal Society of Edinburgh},
  year = {1935},
  volume = {55},
  pages = {42--48},
  abstract = {The seminal Generalised Least Squares contribution, together with
	the first matrix formulation of the Linear Regression Model appeared
	in Aitken's paper, "On Least Squares and Linear Combinations of Observations",
	Proceedings of the Royal Society of Edinburgh, 1935, vol. 55, pp.
	42-48. In this paper we find the well-known extension of the Gauss-Markhov
	Theorem to the case where the regression error vector has a non-scalar
	covariance matrix - the Aitken estimator is "Best Linear Unbiased".
	},
  added-at = {2009-08-08T22:34:52.000+0200},
  biburl = {http://www.bibsonomy.org/bibtex/20996f0eced57ab03ce0799c43edcabf1/peter.ralph},
  booktitle = {Proc. R. Soc. Edinb},
  interhash = {b11fd83f4c5e9f12cfeb3b4cb59e18ae},
  intrahash = {0996f0eced57ab03ce0799c43edcabf1},
  keywords = {weighted_least_squares},
  timestamp = {2009-08-08T22:34:52.000+0200},
  url = {http://scholar.google.com/scholar.bib?q=info:JP6Q2eRpT0QJ:scholar.google.com/&output=citation&hl=en&ct=citation&cd=0}
}

@INPROCEEDINGS{Akaike1973,
  author = {Akaike, H.},
  title = {Information theory and an extension of the maximum likelihood principle},
  booktitle = {Second International Symposium on Information Theory},
  year = {1973},
  editor = {Kotz, S. and Johnson, N. L.},
  pages = {267--281},
  publisher = {Springer-Verlag},
  url = {http://books.google.com/books?hl=en&lr=&id=QN4Dn20r9uoC&oi=fnd&pg=PA199&ots=YX5M0v55eH&sig=tHxqdoeNv1s37a_SbpT-YbglwGU#v=onepage&q&f=false}
}

@ARTICLE{Atkeson1997,
  author = {Atkeson, C. G. and Moore, A. W. and Schaal, S.},
  title = {Locally Weighted Learning},
  journal = {Artificial Intelligence Review},
  year = {1997},
  volume = {11},
  pages = {11--73},
  owner = {jasonb},
  timestamp = {2012.01.10}
}

@INBOOK{Beale1972,
  chapter = {Chapter 4: A Derivation of Conjugate Gradients},
  pages = {39--43},
  title = {Numerical Methods in Nonlinear Optimization},
  publisher = {Academic Press},
  year = {1972},
  editor = {F. A. Lootsma},
  author = {E. M. L. Beale},
  owner = {jasonb},
  timestamp = {2011.12.30}
}

@MANUAL{Bihorel2011,
  title = {{Package: `neldermead': R port of the Scilab neldermead module}},
  author = {S. Bihorel and M. Baudi},
  year = {2011},
  owner = {jasonb},
  timestamp = {2011.12.30}
}

@BOOK{Bishop2007,
  title = {Pattern Recognition and Machine Learning},
  publisher = {Springer},
  year = {2007},
  author = {C. M. Bishop},
  owner = {jasonb},
  timestamp = {2011.12.24}
}

@BOOK{Boyd2004,
  title = {Convex optimization},
  publisher = {Cambridge University Press},
  year = {2004},
  author = {Boyd, S.P. and Vandenberghe, L.},
  isbn = {9780521833783},
  lccn = {2003063284},
  url = {http://books.google.com.au/books?id=mYm0bLd3fcoC}
}

@BOOK{Braun2007,
  title = {A first course in statistical programming with R},
  publisher = {Cambridge University Press},
  year = {2007},
  author = {W. J. Braun and D. J. Murdoch},
  isbn = {9780521872652},
  lccn = {2008295814},
  url = {http://books.google.com.au/books?id=aodgVNrU\_8IC}
}

@TECHREPORT{Breaux1967,
  author = {Breaux, H. J.},
  title = {On stepwise multiple linear regression},
  institution = {Ballistic Research Labs},
  year = {1967},
  number = {1369},
  address = {Aberdeen Proving Ground},
  publisher = {University of Delaware},
  url = {http://books.google.com.au/books?id=HTWySgAACAAJ}
}

@MANUAL{Brehen2011,
  title = {{Package `grpreg'}: Regularization paths for regression models with
	grouped covariates},
  author = {P. Brehen},
  month = {July},
  year = {2011},
  owner = {brownlee},
  timestamp = {2011.12.13},
  url = {http://cran.r-project.org/web/packages/grpreg/grpreg.pdf}
}

@ARTICLE{Breiman2001,
  author = {Leo Breiman},
  title = {Random Forests},
  journal = {Machine Learning},
  year = {2001},
  volume = {45},
  pages = {5--32},
  number = {1},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1023/A:1010933404324}
}

@ARTICLE{Breiman1996,
  author = {Breiman, Leo},
  title = {Bagging predictors},
  journal = {Mach. Learn.},
  year = {1996},
  volume = {24},
  pages = {123--140},
  month = {August},
  acmid = {231989},
  address = {Hingham, MA, USA},
  doi = {10.1023/A:1018054314350},
  issn = {0885-6125},
  issue = {2},
  keywords = {aggregation, averaging, bootstrap, combining},
  numpages = {18},
  publisher = {Kluwer Academic Publishers},
  url = {http://dl.acm.org/citation.cfm?id=231986.231989}
}

@ARTICLE{Breiman1995,
  author = {Breiman, L.},
  title = {Better subset regression using the nonnegative garrote},
  journal = {Technometrics},
  year = {1995},
  volume = {37},
  pages = {373--384},
  month = {November},
  acmid = {219633},
  address = {Alexandria, Va, USA},
  doi = {10.2307/1269730},
  issn = {0040-1706},
  issue = {4},
  keywords = {little bootstrap, model error, prediction, stability},
  numpages = {12},
  publisher = {American Society for Quality Control and American Statistical Association},
  url = {http://dl.acm.org/citation.cfm?id=219631.219633}
}

@TECHREPORT{Breiman1993,
  author = {L. Breiman},
  title = {Better subset selection using the non-negative garotte},
  institution = {University of California, Berkeley},
  year = {1993},
  owner = {brownlee},
  timestamp = {2012.01.30}
}

@TECHREPORT{Breiman2003,
  author = {Leo Breiman and Adele Cutler},
  title = {Manual: Setting up, using, and understanding Random Forests v4.0},
  institution = {University of California, Berkeley},
  year = {2003},
  owner = {brownlee},
  timestamp = {2011.12.09}
}

@BOOK{Breiman1984,
  title = {Classification and regression trees},
  publisher = {Chapman and Hall},
  year = {1984},
  author = {Breiman, Leo and Friedman, J. H. and Olshen, R. A. and Stone, C.
	J.},
  owner = {brownlee},
  timestamp = {2011.11.23}
}

@BOOK{Brent1973,
  title = {Algorithms for Minimization without Derivatives},
  publisher = {Prentice--Hall},
  year = {1973},
  author = {R. Brent},
  owner = {jasonb},
  timestamp = {2011.12.26}
}

@ARTICLE{Broyden1970,
  author = {Broyden, C. G.},
  title = {{The Convergence of a Class of Double-rank Minimization Algorithms
	1. General Considerations}},
  journal = {IMA J Appl Math},
  year = {1970},
  volume = {6},
  pages = {76--90},
  number = {1},
  month = mar,
  abstract = {{This paper presents a more detailed analysis of a class of minimization
	algorithms, which includes as a special case the DFP (Davidon-Fletcher-Powell)
	method, than has previously appeared. Only quadratic functions are
	considered but particular attention is paid to the magnitude of successive
	errors and their dependence upon the initial matrix. On the basis
	of this a possible explanation of some of the observed characteristics
	of the class is tentatively suggested. 10.1093/imamat/6.1.76}},
  citeulike-article-id = {2945939},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/imamat/6.1.76},
  citeulike-linkout-1 = {http://imamat.oxfordjournals.org/cgi/content/abstract/6/1/76},
  day = {1},
  doi = {10.1093/imamat/6.1.76},
  posted-at = {2008-06-30 22:48:42},
  priority = {1},
  url = {http://dx.doi.org/10.1093/imamat/6.1.76}
}

@ARTICLE{Byrd1995,
  author = {Byrd, R. H. and Lu, P. and Nocedal, J. and Zhu, C.},
  title = {A limited memory algorithm for bound constrained optimization},
  journal = {SIAM Journal on Scientific Computing},
  year = {1995},
  volume = {16},
  pages = {1190--1208},
  month = {September},
  acmid = {210980},
  address = {Philadelphia, PA, USA},
  doi = {http://dx.doi.org/10.1137/0916069},
  issn = {1064-8275},
  issue = {5},
  issue_date = {Sept. 1995},
  keywords = {bound constrained optimization, large-scale optimization, limited
	memory method, nonlinear optimization, quasi-Newton method},
  numpages = {19},
  publisher = {Society for Industrial and Applied Mathematics},
  url = {http://dx.doi.org/10.1137/0916069}
}

@ARTICLE{Cauchy1847,
  author = {Cauchy, A--L.},
  title = {{M\'ethode g\'en\'erale pour la r\'esolution des syst\`emes d'\'equations
	simultan\'ees}},
  journal = {Compte Rendu des S\'eances de L'Acad\'emie des Sciences XXV},
  year = {1847},
  volume = {S\'erie A},
  pages = {536--538},
  number = {25},
  month = {October},
  citeulike-article-id = {7133110},
  day = {18},
  keywords = {first, steepest-descent},
  posted-at = {2010-05-07 01:16:11},
  priority = {2}
}

@ARTICLE{Chen1998,
  author = {S. S. Chen and D. Donoho and M. Saunders},
  title = {Atomic decomposition by basis pursuit},
  journal = {SIAM Journal on Scientific Computing},
  year = {1998},
  volume = {20},
  pages = {33--61},
  number = {1},
  owner = {jasonb},
  timestamp = {2012.02.11}
}

@ARTICLE{Cleveland1981,
  author = {W. S. Cleveland},
  title = {{LOWESS: A Program for Smoothing Scatterplots by Robust Locally Weighted
	Regression}},
  journal = {The American Statistician},
  year = {1981},
  volume = {35},
  pages = {54},
  owner = {brownlee},
  timestamp = {2012.01.10}
}

@ARTICLE{Cleveland1979,
  author = {Cleveland, W. S.},
  title = {{Robust Locally Weighted Regression and Smoothing Scatterplots}},
  journal = {Journal of the American Statistical Association},
  year = {1979},
  volume = {74},
  pages = {829--836},
  number = {368},
  abstract = {{The visual information on a scatterplot can be greatly enhanced,
	with little additional cost, by computing and plotting smoothed points.
	Robust locally weighted regression is a method for smoothing a scatterplot,
	(x<sub>i</sub>, y<sub>i</sub>), i = 1, ⋯, n, in which the fitted
	value at x<sub>k</sub> is the value of a polynomial fit to the data
	using weighted least squares, where the weight for (x<sub>i</sub>,
	y<sub>i</sub>) is large if x<sub>i</sub> is close to x<sub>k</sub>
	and small if it is not. A robust fitting procedure is used that guards
	against deviant points distorting the smoothed points. Visual, computational,
	and statistical issues of robust locally weighted regression are
	discussed. Several examples, including data on lead intoxication,
	are used to illustrate the methodology.}},
  citeulike-article-id = {880696},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2286407},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2286407},
  doi = {10.2307/2286407},
  issn = {01621459},
  keywords = {smoothing, statistics},
  posted-at = {2010-06-11 21:10:05},
  priority = {3},
  publisher = {American Statistical Association},
  url = {http://dx.doi.org/10.2307/2286407}
}

@INPROCEEDINGS{Cleveland1978,
  author = {W. S. Cleveland},
  title = {Visual and Computational Considerations in Smoothing Scatterplots
	by Locally Weighted Regression},
  booktitle = {Computer Science and Statistics: Eleventh Annual Symposium on the
	Interface},
  year = {1978},
  pages = {96--100},
  owner = {brownlee},
  timestamp = {2012.01.10}
}

@ARTICLE{Cleveland1988,
  author = {Cleveland, W. S. and Devlin, S. J.},
  title = {{Locally Weighted Regression: An Approach to Regression Analysis
	by Local Fitting}},
  journal = {Journal of the American Statistical Association},
  year = {1988},
  volume = {83},
  pages = {596--610},
  number = {403},
  abstract = {{Locally weighted regression, or loess, is a way of estimating a regression
	surface through a multivariate smoothing procedure, fitting a function
	of the independent variables locally and in a moving fashion analogous
	to how a moving average is computed for a time series. With local
	fitting we can estimate a much wider class of regression surfaces
	than with the usual classes of parametric functions, such as polynomials.
	The goal of this article is to show, through applications, how loess
	can be used for three purposes: data exploration, diagnostic checking
	of parametric models, and providing a nonparametric regression surface.
	Along the way, the following methodology is introduced: (a) a multivariate
	smoothing procedure that is an extension of univariate locally weighted
	regression; (b) statistical procedures that are analogous to those
	used in the least-squares fitting of parametric functions; (c) several
	graphical methods that are useful tools for understanding loess estimates
	and checking the assumptions on which the estimation procedure is
	based; and (d) the M plot, an adaptation of Mallow's C<sub>p</sub>
	procedure, which provides a graphical portrayal of the trade-off
	between variance and bias, and which can be used to choose the amount
	of smoothing.}},
  citeulike-article-id = {3080504},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2289282},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2289282},
  doi = {10.2307/2289282},
  issn = {01621459},
  keywords = {fastrl, learning},
  posted-at = {2008-08-04 10:14:34},
  priority = {2},
  publisher = {American Statistical Association},
  url = {http://dx.doi.org/10.2307/2289282}
}

@ARTICLE{Cleveland1988a,
  author = {W. S. Cleveland and S. J. Devlin and E. Grosse},
  title = {Regression by Local Fitting: Methods, Properties, and Computing},
  journal = {Journal of Econometrics},
  year = {1988},
  volume = {37},
  pages = {87--114},
  number = {1},
  owner = {brownlee},
  timestamp = {2012.01.10}
}

@ARTICLE{Cleveland1991,
  author = {W. S. Cleveland and E. Grosse},
  title = {Computational Methods for Local Regression},
  journal = {Statistics and Computing},
  year = {1991},
  volume = {1},
  pages = {47--62},
  owner = {brownlee},
  timestamp = {2012.01.10}
}

@INBOOK{Cleveland1992,
  chapter = {8: Local regression models},
  pages = {309--376},
  title = {Statistical Models in S},
  publisher = {Chapman and Hall},
  year = {1992},
  editor = {J. M. Chambers and T.J. Hastie},
  author = {W. S. Cleveland and E. Grosse and W. M. Shyu},
  owner = {brownlee},
  timestamp = {2012.01.10}
}

@INBOOK{Cleveland1996,
  chapter = {Statistical Theory and Computational Aspects of Smoothing},
  pages = {10--49},
  title = {Smoothing by Local Regression: Principles and Methods},
  publisher = {Springer},
  year = {1996},
  editor = {W. Haerdle and M. G. Schimek},
  author = {W. S. Cleveland and C. L. Loader},
  owner = {brownlee},
  timestamp = {2012.01.10}
}

@MANUAL{Cortes2011,
  title = {{Package `adabag'}},
  author = {Esteban Alfaro Cortes and Matias Gamez Martinez and Noelia Garcia
	Rubio},
  month = {October},
  year = {2011},
  owner = {brownlee},
  timestamp = {2011.12.12},
  url = {http://cran.r-project.org/web/packages/adabag/adabag.pdf}
}

@BOOK{Crawley2007,
  title = {The R Book},
  publisher = {Wiley},
  year = {2007},
  author = {M. J. Crawley},
  owner = {jasonb},
  timestamp = {2011.12.24}
}

@ARTICLE{Culp2006,
  author = {Mark Culp and Kjell Johnson and George Michailides},
  title = {ada: An R Package for Stochastic Boosting},
  journal = {Journal of Statistical Software},
  year = {2006},
  volume = {17},
  pages = {1--27},
  number = {2},
  month = {9},
  accepted = {2007-07-13},
  bibdate = {2007-07-13},
  coden = {JSSOBK},
  day = {26},
  issn = {1548-7660},
  submitted = {2005-07-13},
  url = {http://www.jstatsoft.org/v17/i02}
}

@MANUAL{Culp2007,
  title = {{Package `ada'}},
  author = {Mark Culp and Kjell Johnson and George Michailidis},
  month = {November},
  year = {2007},
  owner = {brownlee},
  timestamp = {2011.12.12},
  url = {http://cran.r-project.org/web/packages/ada/ada.pdf}
}

@ARTICLE{Curry1944,
  author = {H. B. Curry},
  title = {The Method of Steepest Descent for Non-linear Minimization Problems},
  journal = {Quarterly of Applied Mathematics},
  year = {1944},
  volume = {2},
  pages = {258--261},
  owner = {jasonb},
  timestamp = {2011.12.27}
}

@ARTICLE{Daubechies2004,
  author = {Daubechies, M. and Defrise, and C. De~Mol},
  title = {An iterative thresholding algorithm for linear inverse problems with
	a sparsity constraint},
  journal = {Communications on Pure and Applied Mathematics},
  year = {2004},
  volume = {57},
  pages = {1413--1457},
  owner = {jasonb},
  timestamp = {2012.02.11}
}

@ARTICLE{Derksen1992,
  author = {Derksen, S. and Keselman, H. J.},
  title = {Backward, forward and stepwise automated subset selection algorithms:
	frequency of obtaining authentic and noise variables},
  journal = {British Journal of Mathematical and Statistical Psychology},
  year = {1992},
  volume = {45},
  pages = {265--282},
  number = {2},
  publisher = {British Psychological Society},
  url = {http://cat.inist.fr/?aModele=afficheN&cpsidt=4398639}
}

@TECHREPORT{Devlin1986,
  author = {S. J. Devlin},
  title = {Locally-Weighted Multiple Regression: Statistical Properties and
	Its Use to Test for Linearity},
  institution = {Bell Communications Research},
  year = {1986},
  address = {Pitcataway, NJ},
  owner = {brownlee},
  timestamp = {2012.01.10}
}

@INPROCEEDINGS{Dietterich2000,
  author = {Dietterich, Thomas G.},
  title = {Ensemble Methods in Machine Learning},
  booktitle = {Proceedings of the First International Workshop on Multiple Classifier
	Systems},
  year = {2000},
  series = {MCS '00},
  pages = {1--15},
  address = {London, UK},
  publisher = {Springer-Verlag},
  acmid = {743935},
  isbn = {3-540-67704-6},
  numpages = {15},
  url = {http://dl.acm.org/citation.cfm?id=648054.743935}
}

@TECHREPORT{Dietterich1995,
  author = {Dietterich, T. G., and Kong, E. B.},
  title = {Machine Learning Bias, Statistical Bias, and Statistical Variance
	of Decision Tree Algorithms},
  institution = {Department of Computer Science, Oregon State University},
  year = {1995},
  address = {Corvallis, Oregon},
  owner = {brownlee},
  timestamp = {2011.11.24}
}

@BOOK{Duda2001,
  title = {Pattern Classification},
  publisher = {John Wiley and Sons, Inc.},
  year = {2001},
  author = {R. O. Duda and P. E. Hart and D. G. Stork},
  edition = {Second},
  owner = {brownlee},
  timestamp = {2011.02.09}
}

@ARTICLE{Efron1975,
  author = {Efron, Bradley},
  title = {{The Efficiency of Logistic Regression Compared to Normal Discriminant
	Analysis}},
  journal = {Journal of the American Statistical Association},
  year = {1975},
  volume = {70},
  pages = {892--898},
  number = {352},
  abstract = {{A random vector x arises from one of two multivariate normal distributions
	differing in mean but not covariance. A training set x<sub>1</sub>,
	x<sub>2</sub>, ⋯, x<sub>n</sub> of previous cases, along with their
	correct assignments, is known. These can be used to estimate Fisher's
	discriminant by maximum likelihood and then to assign x on the basis
	of the estimated discriminant, a method known as the normal discrimination
	procedure. Logistic regression does the same thing but with the estimation
	of Fisher's discriminant done conditionally on the observed values
	of x<sub>1</sub>, x<sub>2</sub>, ⋯, x<sub>n</sub>. This article computes
	the asymptotic relative efficiency of the two procedures. Typically,
	logistic regression is shown to be between one half and two thirds
	as effective as normal discrimination for statistically interesting
	values of the parameters.}},
  citeulike-article-id = {9011503},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2285453},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2285453},
  doi = {10.2307/2285453},
  issn = {01621459},
  posted-at = {2011-03-17 22:14:05},
  priority = {2},
  publisher = {American Statistical Association},
  url = {http://dx.doi.org/10.2307/2285453}
}

@ARTICLE{Efron2002,
  author = {Efron, B. and Johnstone, I. and Hastie, T. and Tibshirani, R.},
  title = {Least angle regression},
  journal = {Annals of Statistics},
  year = {2002},
  volume = {32},
  pages = {407--499},
  number = {2},
  owner = {brownlee},
  timestamp = {2012.01.30}
}

@INBOOK{Efroymson1960,
  chapter = {17. Multiple regression analysis},
  pages = {191},
  title = {Mathematical Methods for Digital Computers},
  publisher = {John Wiley \& Sons Inc},
  year = {1960},
  editor = {A. Ralston and H. S. Wilf},
  author = {M. A. Efroymson},
  journal = {Mathematical methods for digital computers},
  owner = {brownlee},
  timestamp = {2012.01.05}
}

@BOOK{Faraway2004,
  title = {{Linear Models with R}},
  publisher = {Chapman and Hall/CRC},
  year = {2004},
  author = {J. Faraway},
  owner = {brownlee},
  timestamp = {2012.01.03}
}

@BOOK{Faraway2006,
  title = {{Extending the linear model with R: generalized linear, mixed effects
	and nonparametric regression models}},
  publisher = {Chapman \& Hall/CRC},
  year = {2006},
  author = {Faraway, J. J.},
  series = {Texts in statistical science},
  isbn = {9781584884248},
  lccn = {2005054822},
  url = {http://books.google.com.au/books?id=ODcRsWpGji4C}
}

@TECHREPORT{Faraway2002,
  author = {J. J. Faraway},
  title = {{Practical Regression and Anova in R}},
  institution = {University of Michigan},
  year = {2002},
  owner = {brownlee},
  timestamp = {2012.01.03},
  url = {http://www.maths.bath.ac.uk/~jjf23/book}
}

@ARTICLE{Fayyad1996a,
  author = {U. Fayyad and G. Piatetsky--Shapiro and P. Smyth},
  title = {From Data Mining to Knowledge Discovery in Databases},
  journal = {AI Mag},
  year = {1996},
  volume = {17},
  number = {3},
  owner = {brownlee},
  timestamp = {2011.02.09}
}

@BOOK{Fletcher2000,
  title = {Practical methods of optimization},
  publisher = {Wiley},
  year = {2000},
  author = {Fletcher, R.},
  series = {A Wiley-Interscience Publication},
  edition = {Second},
  isbn = {9780471494638},
  lccn = {87008126},
  url = {http://books.google.com.au/books?id=LyHAQgAACAAJ}
}

@ARTICLE{Fletcher1970,
  author = {Fletcher, R.},
  title = {{A new approach to variable metric algorithms}},
  journal = {The Computer Journal},
  year = {1970},
  volume = {13},
  pages = {317--322},
  number = {3},
  month = mar,
  abstract = {{An approach to variable metric algorithms has been investigated in
	which the linear search sub-problem no longer becomes necessary.
	The property of quadratic termination has been replaced by one of
	monotonic convergence of the eigenvalues of the approximating matrix
	to the inverse hessian. A convex class of updating formulae which
	possess this property has been established, and a strategy has been
	indicated for choosing a member of the class so as to keep the approximation
	away from both singularity and unboundedness. A FORTRAN program has
	been tested extensively with encouraging results. 10.1093/comjnl/13.3.317}},
  citeulike-article-id = {7133130},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/comjnl/13.3.317},
  day = {1},
  doi = {10.1093/comjnl/13.3.317},
  keywords = {bfgs},
  posted-at = {2010-05-07 01:16:26},
  priority = {2},
  url = {http://dx.doi.org/10.1093/comjnl/13.3.317}
}

@ARTICLE{Fletcher1964,
  author = {Fletcher, R. and Reeves, C. M.},
  title = {Function minimization by conjugate gradients},
  journal = {The Computer Journal},
  year = {1964},
  volume = {7},
  pages = {149--154},
  number = {2},
  publisher = {Br Computer Soc},
  url = {http://comjnl.oupjournals.org/cgi/doi/10.1093/comjnl/7.2.149}
}

@BOOK{Foulkes2009,
  title = {Applied Statistical Genetics with {R}: For Population-Based Association
	Studies},
  publisher = {Springer},
  year = {2009},
  author = {A. S. Foulkes},
  series = {Use R},
  abstract = {In this introductory graduate level text, Dr.~Foulkes elucidates core
	concepts that undergird the wide range of analytic techniques and
	software tools for the analysis of data derived from population-based
	genetic investigations. Applied Statistical Genetics with R offers
	a clear and cogent presentation of several fundamental statistical
	approaches that researchers from multiple disciplines, including
	medicine, public health, epidemiology, statistics and computer science,
	will find useful in exploring this emerging field.},
  isbn = {978-0-387-89553-6},
  orderinfo = {springer.txt},
  publisherurl = {http://www.springeronline.com/978-0-387-89553-6}
}

@ARTICLE{Frawley1992,
  author = {W. J. Frawley and G. Piatetsky--Shapiro and C. J. Matheus},
  title = {Knowledge Discovery in Databases: An Overview},
  journal = {AI Mag},
  year = {1992},
  volume = {13},
  number = {3},
  owner = {brownlee},
  timestamp = {2011.02.09}
}

@ARTICLE{Freund1999,
  author = {Freund, Y. and Schapire, R.},
  title = {{A short introduction to boosting}},
  journal = {J. Japan. Soc. for Artif. Intel.},
  year = {1999},
  volume = {14},
  pages = {771--780},
  number = {5},
  citeulike-article-id = {1626752},
  citeulike-linkout-0 = {\#},
  posted-at = {2007-09-06 15:12:38},
  priority = {0},
  url = {citeseer.ist.psu.edu/freund99short.html}
}

@ARTICLE{Freund1997,
  author = {Yoav Freund and Robert E. Schapire},
  title = {A Decision--Theoretic Generalization of On--Line Learning and an
	Application to Boosting},
  journal = {Journal of Computer and System Sciences},
  year = {1997},
  volume = {55},
  pages = {119--139},
  number = {1},
  doi = {10.1006/jcss.1997.1504},
  issn = {0022-0000},
  url = {http://www.sciencedirect.com/science/article/pii/S002200009791504X}
}

@ARTICLE{Friedman2008a,
  author = {Friedman, J. and Hastie, T, and Tibshirani, R.},
  title = {{Sparse inverse covariance estimation with the graphical lasso}},
  journal = {Biostatistics},
  year = {2008},
  volume = {9},
  pages = {432--441},
  number = {3},
  month = jul,
  abstract = {{We consider the problem of estimating sparse graphs by a lasso penalty
	applied to the inverse covariance matrix. Using a coordinate descent
	procedure for the lasso, we develop a simple algorithm—the graphical
	lasso—that is remarkably fast: It solves a 1000-node problem (∼500000
	parameters) in at most a minute and is 30–4000 times faster than
	competing methods. It also provides a conceptual link between the
	exact problem and the approximation suggested by Meinshausen and
	B\"{u}hlmann (2006). We illustrate the method on some cell-signaling
	data from proteomics.}},
  citeulike-article-id = {2134265},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/biostatistics/kxm045},
  citeulike-linkout-1 = {http://biostatistics.oxfordjournals.org/content/kxm045/.abstract},
  citeulike-linkout-2 = {http://biostatistics.oxfordjournals.org/content/kxm045/.full.pdf},
  citeulike-linkout-3 = {http://biostatistics.oxfordjournals.org/cgi/content/abstract/9/3/432},
  citeulike-linkout-4 = {http://view.ncbi.nlm.nih.gov/pubmed/18079126},
  citeulike-linkout-5 = {http://www.hubmed.org/display.cgi?uids=18079126},
  day = {1},
  doi = {10.1093/biostatistics/kxm045},
  keywords = {bioinformatics, glasso, statistics},
  pmid = {18079126},
  posted-at = {2009-01-17 19:07:07},
  priority = {5},
  url = {http://dx.doi.org/10.1093/biostatistics/kxm045}
}

@ARTICLE{Friedman2007,
  author = {J. Friedman and T. Hastie and H. H\"ofling and R. Tibshirani},
  title = {Pathwise Coordinate Optimization},
  journal = {The Annals of Applied Statistics},
  year = {2007},
  volume = {1},
  pages = {302--332},
  number = {2},
  owner = {jasonb},
  timestamp = {2012.02.11}
}

@MANUAL{Friedman2011,
  title = {{Package `glmnet'}: Lasso and elastic-net regularized generalized
	linear models},
  author = {J. Friedman and T. Hastie and R. Tibshirani},
  year = {2011},
  owner = {brownlee},
  timestamp = {2011.12.13},
  url = {http://cran.r-project.org/web/packages/glmnet/glmnet.pdf}
}

@INBOOK{Friedman1993a,
  chapter = {Estimating Functions of Mixed Ordinal and Categorical Variables Using
	Adaptive Splines},
  pages = {73-113},
  title = {New Directions in Statistical Data Analysis and Robustness},
  publisher = {Verlag},
  year = {1993},
  editor = {S. Morgenthaler and E. Ronchetti and W. A. Stahel},
  author = {Friedman, J. H},
  owner = {brownlee},
  timestamp = {2012.01.09}
}

@ARTICLE{Friedman2001,
  author = {Friedman, J. H.},
  title = {{Greedy function approximation: A gradient boosting machine}},
  journal = {Annals of Statistics},
  year = {2001},
  volume = {29},
  pages = {1189--1232},
  abstract = {{Function approximation is viewed from the perspective of numerical
	optimization in function space, rather than parameter space. A connection
	is made between stagewise additive expansions and steepest--descent
	minimization. A general gradient--descent \&amp;quot;boosting\&amp;quot;
	paradigm is developed for additive expansions based on any fitting
	criterion. Specific algorithms are presented for least--squares,
	least--absolute--deviation, and Huber--M loss functions for regression,
	and multi--class logistic likelihood for classification. Special
	enhancements are derived for the particular case where the individual
	additive components are decision trees, and tools for interpreting
	such \&amp;quot;TreeBoost \&amp;quot; models are presented. Gradient
	boosting of decision trees produces competitive, highly robust, interpretable
	procedures for regression and classification, especially appropriate
	for mining less than clean data. Connections between this approach
	and the boosting methods of Freund and Shapire 1996, and Friedman,
	Hastie, and Tibshirani 1998 are discussed.}},
  citeulike-article-id = {3154111},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.869},
  posted-at = {2008-08-25 20:40:24},
  priority = {2},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.869}
}

@TECHREPORT{Friedman1993,
  author = {Friedman, J. H.},
  title = {{Fast MARS}},
  institution = {Department of Statistics, Stanford University},
  year = {1993},
  number = {110},
  owner = {brownlee},
  timestamp = {2012.01.09}
}

@ARTICLE{Friedman1991,
  author = {Friedman, J. H.},
  title = {{Multivariate Adaptive Regression Splines (with discussion)}},
  journal = {The Annals of Statistics},
  year = {1991},
  volume = {19},
  pages = {1--67},
  number = {1},
  abstract = {{A new method is presented for flexible regression modeling of high
	dimensional data. The model takes the form of an expansion in product
	spline basis functions, where the number of basis functions as well
	as the parameters associated with each one (product degree and knot
	locations) are automatically determined by the data. This procedure
	is motivated by the recursive partitioning approach to regression
	and shares its attractive properties. Unlike recursive partitioning,
	however, this method produces continuous models with continuous derivatives.
	It has more power and flexibility to model relationships that are
	nearly additive or involve interactions in at most a few variables.
	In addition, the model can be represented in a form that separately
	identifies the additive contributions and those associated with the
	different multivariable interactions.}},
  citeulike-article-id = {4371368},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2241837},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2241837},
  doi = {10.2307/2241837},
  issn = {00905364},
  keywords = {nonparametric\_regression, statistics},
  posted-at = {2009-04-21 09:08:01},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  url = {http://dx.doi.org/10.2307/2241837}
}

@TECHREPORT{Friedman1991a,
  author = {J. H. Friedman},
  title = {Estimating Functions of Mixed Ordinal and Categorical Variables using
	Adaptive Splines},
  institution = {Stanford University},
  year = {1991},
  number = {LCS 109},
  owner = {brownlee},
  timestamp = {2012.01.09}
}

@TECHREPORT{Friedman1991b,
  author = {J. H. Friedman},
  title = {Adaptive Spline Networks},
  institution = {Stanford University},
  year = {1991},
  number = {LCS 107},
  owner = {brownlee},
  timestamp = {2012.01.09}
}

@INPROCEEDINGS{Friedman1991c,
  author = {Friedman, J. H.},
  title = {Adaptive spline networks},
  booktitle = {Proceedings of the 1990 conference on Advances in neural information
	processing systems 3},
  year = {1991},
  series = {NIPS-3},
  pages = {675--683},
  address = {San Francisco, CA, USA},
  publisher = {Morgan Kaufmann Publishers Inc.},
  acmid = {118979},
  isbn = {1-55860-184-8},
  location = {Denver, Colorado, United States},
  numpages = {9},
  url = {http://dl.acm.org/citation.cfm?id=118850.118979}
}

@ARTICLE{Friedman2010,
  author = {J. H. Friedman and T. Hastie and R. Tibshirani},
  title = {Regularization Paths for Generalized Linear Models via Coordinate
	Descent},
  journal = {Journal of Statistical Software},
  year = {2010},
  volume = {33},
  pages = {1--22},
  number = {1},
  month = {2},
  accepted = {2009-12-15},
  bibdate = {2009-12-15},
  coden = {JSSOBK},
  day = {2},
  issn = {1548-7660},
  submitted = {2009-04-22},
  url = {http://www.jstatsoft.org/v33/i01}
}

@ARTICLE{Friedman1995,
  author = {Friedman, J. H. and Roosen, C. B.},
  title = {An introduction to multivariate adaptive regression splines.},
  journal = {Statistical Methods in Medical Research},
  year = {1995},
  volume = {4},
  pages = {197--217},
  number = {3},
  month = {Sep},
  __markedentry = {[brownlee:6]},
  abstract = {Multivariate Adaptive Regression Splines (MARS) is a method for flexible
	modelling of high dimensional data. The model takes the form of an
	expansion in product spline basis functions, where the number of
	basis functions as well as the parameters associated with each one
	(product degree and knot locations) are automatically determined
	by the data. This procedure is motivated by recursive partitioning
	(e.g. CART) and shares its ability to capture high order interactions.
	However, it has more power and flexibility to model relationships
	that are nearly additive or involve interactions in at most a few
	variables, and produces continuous models with continuous derivatives.
	In addition, the model can be represented in a form that separately
	identifies the additive contributions and those associated with different
	multivariable interactions. This paper summarizes the basic MARS
	algorithm, as well as extensions for binary response, categorical
	predictors, nested variables and missing values. It presents tips
	on interpreting the output of the standard FORTRAN implementation
	of MARS, and provides an example of MARS applied to a set of clinical
	data.},
  institution = {Department of Statistics and Stanford Linear Accelerator Center,
	Stanford University, CA 94305-4065, USA.},
  keywords = {Algorithms; Analysis of Variance; Computer Graphics; Data Interpretation,
	Statistical; Humans; Linear Models; Logistic Models; Multivariate
	Analysis; Myocardial Infarction, mortality; Programming Languages;
	Regression Analysis; Reproducibility of Results; Statistics, Nonparametric;
	Survival Analysis},
  language = {eng},
  medline-pst = {ppublish},
  owner = {brownlee},
  pmid = {8548103},
  timestamp = {2012.01.06}
}

@ARTICLE{Fu1998,
  author = {Fu, W. J.},
  title = {{Penalized Regressions: The Bridge versus the Lasso}},
  journal = {Journal of Computational and Graphical Statistics},
  year = {1998},
  volume = {7},
  pages = {397--416},
  number = {3},
  abstract = {{Bridge regression, a special family of penalized regressions of a
	penalty function ∑|β <sub>j</sub>|<sup>γ</sup> with γ ≥ 1, is considered.
	A general approach to solve for the bridge estimator is developed.
	A new algorithm for the lasso (γ = 1) is obtained by studying the
	structure of the bridge estimators. The shrinkage parameter γ and
	the tuning parameter λ are selected via generalized cross-validation
	(GCV). Comparison between the bridge model (γ ≥ 1) and several other
	shrinkage models, namely the ordinary least squares regression (λ
	= 0), the lasso (γ = 1) and ridge regression (γ = 2), is made through
	a simulation study. It is shown that the bridge regression performs
	well compared to the lasso and ridge regression. These methods are
	demonstrated through an analysis of a prostate cancer data. Some
	computational advantages and limitations are discussed.}},
  citeulike-article-id = {6761056},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/1390712},
  citeulike-linkout-1 = {http://www.jstor.org/stable/1390712},
  doi = {10.2307/1390712},
  issn = {10618600},
  keywords = {cross\_validation, penalized\_regression, shooting\_method},
  posted-at = {2010-07-11 14:39:14},
  priority = {2},
  publisher = {American Statistical Association, Institute of Mathematical Statistics,
	and Interface Foundation of America},
  url = {http://dx.doi.org/10.2307/1390712}
}

@BOOK{Fukunaga1990,
  title = {Introduction to Statistical Pattern Recognition},
  publisher = {Academic Press},
  year = {1990},
  author = {K. Fukunaga},
  edition = {Second},
  owner = {jbrownlee},
  timestamp = {2008.03.04}
}

@BOOK{Gauss1823,
  title = {Theoria combinationis observationum erroribus minimis obnoxiae},
  publisher = {H. Dieterich},
  year = {1823},
  author = {Gauss, C. F.},
  url = {http://books.google.co.za/books?id=ZQ8OAAAAQAAJ}
}

@BOOK{Gauss1809,
  title = {Theoria motus corporum coelestium in sectionibus conicis solem ambientium},
  year = {1809},
  author = {Gauss, C. F.},
  url = {http://books.google.com.au/books?id=ORUOAAAAQAAJ}
}

@ARTICLE{Goldfarb1970,
  author = {Goldfarb, D.},
  title = {{A Family of Variable Metric Updates Derived by Variational Means}},
  journal = {Mathematics of Computation},
  year = {1970},
  volume = {24},
  pages = {23--26},
  citeulike-article-id = {8491642},
  posted-at = {2010-12-29 19:26:05},
  priority = {2}
}

@BOOK{Griva2009,
  title = {Linear and nonlinear optimization},
  publisher = {Society for Industrial and Applied Mathematics},
  year = {2009},
  author = {Griva, I. and Nash, S.G. and Sofer, A.},
  isbn = {9780898716610},
  lccn = {2008032477},
  url = {http://books.google.com.au/books?id=uOJ-Vg1BnKgC}
}

@MANUAL{Hankin2011,
  title = {{Package `gsl': wrapper for the Gnu Scientific Library}},
  author = {R. K. S. Hankin and D. Murdoch and A. Clausen},
  year = {2011},
  owner = {jasonb},
  timestamp = {2011.12.30}
}

@MANUAL{Hastie2011,
  title = {{Package `lars': Least Angle Regression, Lasso and Forward Stagewise}},
  author = {T. Hastie and B. Efron},
  month = {March},
  year = {2011},
  owner = {brownlee},
  timestamp = {2011.12.13},
  url = {http://cran.r-project.org/web/packages/lars/lars.pdf}
}

@ARTICLE{Hastie1986,
  author = {T. Hastie and R. Tibshirani},
  title = {Generalized Additive Models},
  journal = {Statistical Science},
  year = {1986},
  volume = {1},
  pages = {297--318},
  number = {3},
  owner = {brownlee},
  timestamp = {2012.01.10}
}

@BOOK{Hastie2009,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and
	Prediction},
  publisher = {Springer},
  year = {2009},
  author = {T. Hastie and R. Tibshirani and J. Friedman},
  edition = {Second},
  owner = {brownlee},
  timestamp = {2011.02.08}
}

@BOOK{Hastie1990,
  title = {Generalized Additive Models},
  publisher = {{Chapman \& Hall/CRC}},
  year = {1990},
  author = {Hastie, T. J. and Tibshirani, R. J.},
  owner = {brownlee},
  timestamp = {2012.01.10}
}

@BOOK{Heath2002,
  title = {Scientific computing: an introductory survey},
  publisher = {McGraw-Hill},
  year = {2002},
  author = {M. T. Heath},
  owner = {jasonb},
  timestamp = {2011.12.26}
}

@BOOK{Hestenes1980,
  title = {Conjugate direction methods in optimization},
  publisher = {Springer-Verlag},
  year = {1980},
  author = {Hestenes, M. R.},
  series = {Applications of mathematics},
  isbn = {9780387904559},
  lccn = {79020220},
  url = {http://books.google.com.au/books?id=hUzvAAAAMAAJ}
}

@ARTICLE{Hestenes1952,
  author = {Hestenes, M. R. and Stiefel, E.},
  title = {Methods of conjugate gradients for solving linear systems},
  journal = {Journal Of Research Of The National Bureau Of Standards},
  year = {1952},
  volume = {49},
  pages = {409--436},
  number = {6},
  url = {http://www.stat.uchicago.edu/~lekheng/courses/302/classics/hestenes-stiefel.pdf}
}

@ARTICLE{Hocking1976,
  author = {Hocking, R. R.},
  title = {The Analysis and Selection of Variables in Linear Regression},
  journal = {Biometrics},
  year = {1976},
  volume = {32},
  pages = {1--49},
  number = {1},
  url = {http://www.jstor.org/stable/2529336}
}

@ARTICLE{Hoerl1962,
  author = {Hoerl, A. E.},
  title = {Application of ridge analysis to regression problems},
  journal = {Chemical Engineering Progress},
  year = {1962},
  volume = {58},
  pages = {54–59},
  number = {3},
  url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Application+of+ridge+analysis+to+regresion+problems#0}
}

@ARTICLE{Hoerl1970,
  author = {Hoerl, A. E. and Kennard, R. W.},
  title = {{Ridge Regression: Biased Estimation for Nonorthogonal Problems}},
  journal = {Technometrics},
  year = {1970},
  volume = {12},
  pages = {55--67},
  number = {1},
  abstract = {{In multiple regression it is shown that parameter estimates based
	on minimum residual sum of squares have a high probability of being
	unsatisfactory, if not incorrect, if the prediction vectors are not
	orthogonal. Proposed is an estimation procedure based on adding small
	positive quantities to the diagonal of X′X. Introduced is the ridge
	trace, a method for showing in two dimensions the effects of nonorthogonality.
	It is then shown how to augment X′X to obtain biased estimates with
	smaller mean square error.}},
  citeulike-article-id = {5831046},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/1267351},
  citeulike-linkout-1 = {http://www.jstor.org/stable/1267351},
  doi = {10.2307/1267351},
  issn = {00401706},
  keywords = {prediction, regression, regularization, statistics},
  posted-at = {2010-04-15 09:54:50},
  priority = {0},
  publisher = {American Statistical Association and American Society for Quality},
  url = {http://dx.doi.org/10.2307/1267351}
}

@ARTICLE{Hoerl1970a,
  author = {Hoerl, A. E. and Kennard, R. W.},
  title = {{Ridge Regression: Applications to Nonorthogonal Problems}},
  journal = {Technometrics},
  year = {1970},
  volume = {12},
  pages = {69--82},
  number = {1},
  publisher = {JSTOR},
  url = {http://www.jstor.org/stable/1267352?origin=crossref}
}

@ARTICLE{Hooke1961,
  author = {Hooke, R. and Jeeves, T. A.},
  title = {{``Direct Search'' Solution of Numerical and Statistical Problems}},
  journal = {Journal of the ACM},
  year = {1961},
  volume = {8},
  pages = {212--229},
  number = {2},
  month = apr,
  address = {New York, NY, USA},
  citeulike-article-id = {824801},
  citeulike-linkout-0 = {http://dx.doi.org/10.1145/321062.321069},
  doi = {10.1145/321062.321069},
  issn = {0004-5411},
  keywords = {global\_optimization},
  posted-at = {2006-09-01 08:57:26},
  priority = {2},
  publisher = {ACM Press},
  url = {http://dx.doi.org/10.1145/321062.321069}
}

@BOOK{Hosmer2000,
  title = {Applied logistic regression},
  publisher = {Wiley},
  year = {2000},
  author = {Hosmer, D. W. and Lemeshow, S.},
  series = {Wiley series in probability and statistics: Texts and references
	section},
  isbn = {9780471356325},
  lccn = {00036843},
  url = {http://books.google.com.au/books?id=DEtar8K5ASsC}
}

@TECHREPORT{Johnson1955,
  author = {S. M. Johnson},
  title = {Best exploration for maximum is Fibonaccian},
  institution = {RAND Corporation},
  year = {1955},
  type = {Research Memoranda},
  number = {RM--1590},
  address = {Santa Monica, Calif},
  month = {November},
  publisher = {Rand Corporation},
  url = {http://books.google.com.au/books?id=3uJ0HAAACAAJ}
}

@ARTICLE{Karatzoglou2006,
  author = {Alexandros Karatzoglou and David Meyer and Kurt Hornik},
  title = {Support Vector Machines in R},
  journal = {Journal of Statistical Software},
  year = {2006},
  volume = {15},
  pages = {1--28},
  number = {9},
  month = {4},
  accepted = {2006-04-06},
  bibdate = {2006-04-06},
  coden = {JSSOBK},
  day = {6},
  issn = {1548-7660},
  submitted = {2005-10-24},
  url = {http://www.jstatsoft.org/v15/i09}
}

@BOOK{Kearns1994,
  title = {An introduction to computational learning theory},
  publisher = {MIT Press},
  year = {1994},
  author = {M. J. Kearns and U. V. Vazirani},
  owner = {brownlee},
  timestamp = {2011.02.09}
}

@ARTICLE{Kiefer1953,
  author = {J. Kiefer},
  title = {Sequential minimax search for a maximum},
  journal = {Proceedings of the American Mathematical Society},
  year = {1953},
  volume = {4},
  pages = {502--506},
  number = {3},
  owner = {jasonb},
  timestamp = {2011.12.27}
}

@ARTICLE{Kim2006,
  author = {Kim, Y. and Kim, J. and Kim, Y.},
  title = {{Blockwise Sparse Regression}},
  journal = {Statistica Sinica},
  year = {2006},
  volume = {16},
  pages = {375--390},
  citeulike-article-id = {5442498},
  keywords = {file-import-09-08-15, sparse},
  posted-at = {2009-08-15 09:07:29},
  priority = {2}
}

@BOOK{Kleinbaum2010,
  title = {Logistic Regression: A Self-Learning Text},
  publisher = {Springer},
  year = {2010},
  author = {Kleinbaum, D. G. and Klein, M. and Pryor, E. R.},
  series = {Statistics for Biology and Health},
  isbn = {9781441917416},
  lccn = {2009943538},
  url = {http://books.google.com.au/books?id=J7E0JQweHkoC}
}

@TECHREPORT{Komarek2005,
  author = {P. Komarek and A. Moore},
  title = {Making Logistic Regression A Core Data Mining Tool: A Practical Investigation
	of Accuracy, Speed, and Simplicity},
  institution = {Robotics Institute, Carnegie Mellon University},
  year = {2005},
  number = {CMU-RI-TR-05-27},
  address = {Pittsburgh, PA},
  month = {May},
  howpublished = {technical report},
  pages = {13},
  volume = {TR-05-27}
}

@MANUAL{Kuhn2011,
  title = {{Package `caret'}},
  author = {Max Kuhn},
  month = {December},
  year = {2011},
  owner = {brownlee},
  timestamp = {2011.12.12},
  url = {http://cran.r-project.org/web/packages/caret/caret.pdf}
}

@ARTICLE{Lagarias1998,
  author = {Lagarias, J. C. and Reeds, J. A. and Wright, M. H. and Wright, P.
	E.},
  title = {Convergence Properties of the Nelder--Mead Simplex Method in Low
	Dimensions},
  journal = {SIAM Journal on Optimization},
  year = {1998},
  volume = {9},
  pages = {112--147},
  month = {May},
  acmid = {589108},
  address = {Philadelphia, PA, USA},
  doi = {http://dx.doi.org/10.1137/S1052623496303470},
  issn = {1052-6234},
  issue = {1},
  keywords = {Nelder--Mead simplex methods, direct search methods, nonderivative
	optimization},
  numpages = {36},
  publisher = {Society for Industrial and Applied Mathematics},
  url = {http://dx.doi.org/10.1137/S1052623496303470}
}

@BOOK{Legendre1805,
  title = {Nouvelles m{\'e}thodes pour la d{\'e}termination des orbites des
	com{\`e}tes},
  publisher = {F. Didot},
  year = {1805},
  author = {Legendre, A. M.},
  lccn = {nuc87594942},
  url = {http://books.google.com.au/books?id=spcAAAAAMAAJ}
}

@ARTICLE{Lewis2000,
  author = {Lewis, R. M. and Torczon, V. and Trosset, M. W.},
  title = {{Direct search methods: then and now}},
  journal = {Journal of Computational and Applied Mathematics},
  year = {2000},
  volume = {124},
  pages = {191--207},
  number = {1-2},
  month = dec,
  abstract = {{We discuss direct search methods for unconstrained optimization.
	We give a modern perspective on this classical family of derivative-free
	algorithms, focusing on the development of direct search methods
	during their golden age from 1960 to 1971. We discuss how direct
	search methods are characterized by the absence of the construction
	of a model of the objective. We then consider a number of the classical
	direct search methods and discuss what research in the intervening
	years has uncovered about these algorithms. In particular, while
	the original direct search methods were consciously based on straightforward
	heuristics, more recent analysis has shown that in most -- but not
	all -- cases these heuristics actually suffice to ensure global convergence
	of at least one subsequence of the sequence of iterates to a first-order
	stationary point of the objective function.}},
  citeulike-article-id = {824833},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/S0377-0427(00)00423-4},
  citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6TYH-41MJ0RK-C/2/bbf6e8fa1fb38e23fd611727efc9e67d},
  day = {1},
  doi = {10.1016/S0377-0427(00)00423-4},
  keywords = {global\_optimization},
  posted-at = {2006-09-01 09:57:49},
  priority = {2},
  url = {http://dx.doi.org/10.1016/S0377-0427(00)00423-4}
}

@ARTICLE{Liaw2002,
  author = {Andy Liaw and Matthew Wiener},
  title = {Classiﬁcation and Regression by randomForest},
  journal = {R News},
  year = {2002},
  volume = {2},
  pages = {18--22},
  number = {3},
  month = {December},
  owner = {brownlee},
  timestamp = {2011.12.09}
}

@ARTICLE{Liu1989,
  author = {Liu, D. C. and Nocedal, J.},
  title = {On the limited memory BFGS method for large scale optimization},
  journal = {Mathematical Programming},
  year = {1989},
  volume = {45},
  pages = {503--528},
  month = {December},
  acmid = {83726},
  address = {Secaucus, NJ, USA},
  doi = {10.1007/BF01589116},
  issn = {0025-5610},
  issue = {3},
  numpages = {26},
  publisher = {Springer-Verlag New York, Inc.},
  url = {http://dl.acm.org/citation.cfm?id=81100.83726}
}

@ARTICLE{Long2010,
  author = {Long, Philip M. and Servedio, Rocco A.},
  title = {Random classification noise defeats all convex potential boosters},
  journal = {Mach. Learn.},
  year = {2010},
  volume = {78},
  pages = {287--304},
  month = {March},
  acmid = {1713653},
  address = {Hingham, MA, USA},
  doi = {http://dx.doi.org/10.1007/s10994-009-5165-z},
  issn = {0885-6125},
  issue = {3},
  keywords = {Boosting, Convex loss, Learning theory, Misclassification noise, Noise-tolerant
	learning, Potential boosting},
  numpages = {18},
  publisher = {Kluwer Academic Publishers},
  url = {http://dx.doi.org/10.1007/s10994-009-5165-z}
}

@BOOK{Luger1993,
  title = {Artificial Intelligence: Structures and Strategies for Complex Problem
	Solving},
  publisher = {Benjamin/Cummings Pub. Co.},
  year = {1993},
  author = {G. F. Luger and W. A. Stubblefield},
  edition = {Second},
  owner = {jasonb},
  timestamp = {2010.01.13}
}

@BOOK{MacKay2003,
  title = {Information theory, inference, and learning algorithms},
  publisher = {Cambridge University Press},
  year = {2003},
  author = {D. J. C. MacKay},
  owner = {brownlee},
  timestamp = {2011.02.09}
}

@BOOK{Matloff2011,
  title = {The Art of R Programming: A Tour of Statistical Software Design},
  publisher = {No Starch Press},
  year = {2011},
  author = {N. Matloff},
  owner = {jasonb},
  timestamp = {2011.12.24}
}

@BOOK{McCullagh1989,
  title = {Generalized Linear Models},
  publisher = {Chapman and Hall/CRC},
  year = {1989},
  author = {McCullagh, P. and Nelder, J. A.},
  edition = {Second},
  isbn = {9780412317606},
  url = {http://books.google.com.au/books?id=h9kFH2\_FfBkC}
}

@ARTICLE{McCulloch2000,
  author = {C. E. McCulloch},
  title = {Generalized Linear Models},
  journal = {Journal of the American Statistical Association},
  year = {2000},
  volume = {95},
  pages = {1320--1324},
  owner = {brownlee},
  timestamp = {2012.01.04}
}

@ARTICLE{Mease2008,
  author = {Mease, David and Wyner, Abraham},
  title = {{Evidence Contrary to the Statistical View of Boosting}},
  journal = {Journal of Machine Learning Research},
  year = {2008},
  volume = {9},
  pages = {131--156},
  abstract = {{The statistical perspective on boosting algorithms focuses on optimization,
	drawing parallels with maximum likelihood estimation for logistic
	regression. In this paper we present empirical evidence that raises
	questions about this view. Although the statistical perspective provides
	a theoretical framework within which it is possible to derive theorems
	and create new algorithms in general contexts, we show that there
	remain many unanswered important questions. Furthermore, we provide
	examples that reveal crucial flaws in the many practical suggestions
	and new methods that are derived from the statistical view. We perform
	carefully designed experiments using simple simulation models to
	illustrate some of these flaws and their practical consequences.}},
  address = {Cambridge, MA, USA},
  citeulike-article-id = {5128162},
  citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1390681.1390687},
  issn = {1532-4435},
  keywords = {boosting, machine\_learning, simulation, statistics},
  posted-at = {2009-07-12 19:15:25},
  priority = {2},
  publisher = {JMLR.org},
  url = {http://portal.acm.org/citation.cfm?id=1390681.1390687}
}

@MANUAL{Meier2009,
  title = {{Package `grplasso'}: Fitting user speciﬁed models with Group Lasso
	penalty},
  author = {L. Meier},
  year = {2009},
  owner = {brownlee},
  timestamp = {2011.12.13},
  url = {http://cran.r-project.org/web/packages/grplasso/grplasso.pdf}
}

@INBOOK{Meir2003,
  pages = {118--183},
  title = {An introduction to boosting and leveraging},
  publisher = {Springer-Verlag New York, Inc.},
  year = {2003},
  author = {Meir, Ron and R\"{a}tsch, Gunnar},
  address = {New York, NY, USA},
  acmid = {863719},
  book = {Advanced lectures on machine learning},
  isbn = {3-540-00529-3},
  numpages = {66},
  url = {http://dl.acm.org/citation.cfm?id=863714.863719}
}

@TECHREPORT{Meyer2011,
  author = {David Meyer},
  title = {Support Vector Machines: The Interface to libsvm in package e1071},
  institution = {Technische Universitat Wien, Austria},
  year = {2011},
  owner = {brownlee},
  timestamp = {2011.12.06}
}

@MANUAL{Milborrow2012,
  title = {{Package `earth': Multivariate Adaptive Regression Spline Models}},
  author = {S. Milborrow},
  year = {2012},
  owner = {jasonb},
  timestamp = {2012.01.09},
  url = {http://cran.r-project.org/web/packages/earth/earth.pdf}
}

@MANUAL{Milborrow2011,
  title = {Notes on the earth package},
  author = {S. Milborrow},
  year = {2011},
  owner = {jasonb},
  timestamp = {2012.01.09},
  url = {http://cran.r-project.org/web/packages/earth/vignettes/earth-notes.pdf}
}

@ARTICLE{Miller1996,
  author = {Miller, A. J.},
  title = {{The convergence of Efroymson's stepwise regression algorithm}},
  journal = {The American Statistician},
  year = {1996},
  volume = {50},
  owner = {brownlee},
  timestamp = {2012.01.05}
}

@ARTICLE{Miller1984,
  author = {Miller, A. J.},
  title = {{Selection of Subsets of Regression Variables}},
  journal = {Journal of the Royal Statistical Society. Series A (General)},
  year = {1984},
  volume = {147},
  pages = {389--425},
  number = {3},
  abstract = {{Computational algorithms for selecting subsets of regression variables
	are discussed. Only linear models and the least-squares criterion
	are considered. The use of planar-rotation algorithms, instead of
	Gauss-Jordan methods, is advocated. The advantages and disadvantages
	of a number of "cheap" search methods are described for use when
	it is not feasible to carry out an exhaustive search for the best-fitting
	subsets. Hypothesis testing for three purposes is considered, namely
	(i) testing for zero regression coefficients for remaining variables,
	(ii) comparing subsets and (iii) testing for any predictive value
	in a selected subset. Three small data sets are used to illustrate
	these test. Spj{\o}tvoll's (1972a) test is discussed in detail, though
	an extension to this test appears desirable. Estimation problems
	have largely been overlooked in the past. Three types of bias are
	identified, namely that due to the omission of variables, that due
	to competition for selection and that due to the stopping rule. The
	emphasis here is on competition bias, which can be of the order of
	two or more standard errors when coefficients are estimated from
	the same data as were used to select the subset. Five possible ways
	of handling this bias are listed. This is the area most urgently
	requiring further research. Mean squared errors of prediction and
	stopping rules are briefly discussed. Competition bias invalidates
	the use of existing stopping rules as they are commonly applied to
	try to produce optimal prediction equations.}},
  citeulike-article-id = {8271080},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2981576},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2981576},
  doi = {10.2307/2981576},
  issn = {00359238},
  keywords = {model\_selection},
  posted-at = {2011-03-24 18:46:07},
  priority = {2},
  publisher = {Blackwell Publishing for the Royal Statistical Society},
  url = {http://dx.doi.org/10.2307/2981576}
}

@ARTICLE{Mills2010,
  author = {Mills, J.},
  title = {{Estimation of Statistical Models in R}},
  journal = {Syntax},
  year = {2010},
  pages = {1--145}
}

@TECHREPORT{Mitchell2006,
  author = {T. Mitchell},
  title = {The Discipline of Machine Learning},
  institution = {School of Computer Science, Carnegie Mellon University},
  year = {2006},
  number = {CMU-ML-06-108},
  month = {July},
  owner = {brownlee},
  timestamp = {2011.02.08}
}

@BOOK{Mitchell1997,
  title = {Machine Learning},
  publisher = {McGraw-Hill},
  year = {1997},
  author = {T. M. Mitchell},
  owner = {brownlee},
  timestamp = {2011.02.08}
}

@BOOK{Montgomery2001,
  title = {Introduction to Linear Regression Analysis},
  publisher = {Wiley-Interscience},
  year = {2001},
  author = {Montgomery, D. C. and Peck, E. A. and Vining, G. G.},
  series = {Wiley Series in Probability and Statistics},
  edition = {Third},
  isbn = {9780470542811},
  url = {http://books.google.com.au/books?id=LwihcQAACAAJ}
}

@ARTICLE{Mundry2009,
  author = {Mundry, R. and Nunn, C. L.},
  title = {Stepwise model fitting and statistical inference: turning noise into
	signal pollution.},
  journal = {The American naturalist},
  year = {2009},
  volume = {173},
  pages = {119--23},
  number = {1},
  publisher = {UChicago Press},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/19049440}
}

@ARTICLE{Nelder1965,
  author = {Nelder, J. A. and Mead, R.},
  title = {{A Simplex Method for Function Minimization}},
  journal = {The Computer Journal},
  year = {1965},
  volume = {7},
  pages = {308--313},
  number = {4},
  month = jan,
  abstract = {{A method is described for the minimization of a function of n variables,
	which depends on the comparison of function values at the (n + 1)
	vertices of a general simplex, followed by the replacement of the
	vertex with the highest value by another point. The simplex adapts
	itself to the local landscape, and contracts on to the final minimum.
	The method is shown to be effective and computationally compact.
	A procedure is given for the estimation of the Hessian matrix in
	the neighbourhood of the minimum, needed in statistical estimation
	problems.}},
  citeulike-article-id = {3009487},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/comjnl/7.4.308},
  citeulike-linkout-1 = {http://comjnl.oxfordjournals.org/content/7/4/308.abstract},
  citeulike-linkout-2 = {http://comjnl.oxfordjournals.org/content/7/4/308.full.pdf},
  citeulike-linkout-3 = {http://comjnl.oxfordjournals.org/cgi/content/abstract/7/4/308},
  day = {1},
  doi = {10.1093/comjnl/7.4.308},
  posted-at = {2008-09-15 16:23:09},
  priority = {2},
  url = {http://dx.doi.org/10.1093/comjnl/7.4.308}
}

@ARTICLE{Nelder1972,
  author = {Nelder, J. A. and Wedderburn, R. W. M.},
  title = {{Generalized Linear Models}},
  journal = {Journal of the Royal Statistical Society. Series A (General)},
  year = {1972},
  volume = {135},
  pages = {370--384},
  number = {3},
  abstract = {{The technique of iterative weighted linear regression can be used
	to obtain maximum likelihood estimates of the parameters with observations
	distributed according to some exponential family and systematic effects
	that can be made linear by a suitable transformation. A generalization
	of the analysis of variance is given for these models using log-likelihoods.
	These generalized linear models are illustrated by examples relating
	to four distributions; the Normal, Binomial (probit analysis, etc.),
	Poisson (contingency tables) and gamma (variance components). The
	implications of the approach in designing statistics courses are
	discussed.}},
  citeulike-article-id = {5485398},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2344614},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2344614},
  doi = {10.2307/2344614},
  issn = {00359238},
  keywords = {generalized-linear-models, glm, linear-models, statistics},
  posted-at = {2009-08-19 09:11:12},
  priority = {2},
  publisher = {Blackwell Publishing for the Royal Statistical Society},
  url = {http://dx.doi.org/10.2307/2344614}
}

@ARTICLE{Nocedal1980,
  author = {Nocedal, J.},
  title = {{Updating Quasi-Newton Matrices with Limited Storage}},
  journal = {Mathematics of Computation},
  year = {1980},
  volume = {35},
  pages = {773--782},
  number = {151},
  abstract = {{We study how to use the BFGS quasi-Newton matrices to precondition
	minimization methods for problems where the storage is critical.
	We give an update formula which generates matrices using information
	from the last m iterations, where m is any number supplied by the
	user. The quasi-Newton matrix is updated at every iteration by dropping
	the oldest information and replacing it by the newest information.
	It is shown that the matrices generated have some desirable properties.
	The resulting algorithms are tested numerically and compared with
	several well-known methods.}},
  citeulike-article-id = {1284223},
  citeulike-linkout-0 = {http://www.jstor.org/stable/2006193},
  keywords = {algorithm, maximum-entropy, optimization},
  posted-at = {2008-01-29 08:53:48},
  priority = {2},
  url = {http://www.jstor.org/stable/2006193}
}

@BOOK{Nocedal1999,
  title = {Numerical optimization},
  publisher = {Springer},
  year = {1999},
  author = {Nocedal, J. and Wright, S. J.},
  series = {Springer series in operations research},
  isbn = {9780387987934},
  lccn = {99013263},
  url = {http://books.google.com.au/books?id=epc5fX0lqRIC}
}

@ARTICLE{Osborne2000,
  author = {M. R. Osborne and B. Presnell and B. A. Turlach},
  title = {A new approach to variable selection in least squares problems},
  journal = {IMA Journal of Numerical Analysis},
  year = {2000},
  volume = {20},
  pages = {389--403},
  number = {3},
  owner = {jasonb},
  timestamp = {2012.02.11}
}

@ARTICLE{Overholt1965,
  author = {K. J. Overholt},
  title = {An instability in the Fibonacci and golden section search methods},
  journal = {BIT},
  year = {1965},
  volume = {5},
  pages = {284},
  owner = {jasonb},
  timestamp = {2011.12.26}
}

@BOOK{Pampel2000,
  title = {Logistic regression: a primer},
  publisher = {Sage Publications},
  year = {2000},
  author = {Pampel, F. C.},
  series = {Sage university papers series: Quantitative applications in the social
	sciences},
  isbn = {9780761920106},
  lccn = {00008060},
  url = {http://books.google.com.au/books?id=lfzSqxFceq0C}
}

@ARTICLE{Peduzzi1996,
  author = {Peduzzi, P. and Concato, J. and Kemper, E. and Holford, T. R. and
	Feinstein, A. R.},
  title = {{A simulation study of the number of events per variable in logistic
	regression analysis}},
  journal = {Journal of Clinical Epidemiology},
  year = {1996},
  volume = {49},
  pages = {1373--1379},
  number = {12},
  month = dec,
  abstract = {{We performed a Monte Carlo study to evaluate the effect of the number
	of events per variable (EPV) analyzed in logistic regression analysis.
	The simulations were based on data from a cardiac trial of 673 patients
	in which 252 deaths occurred and seven variables were cogent predictors
	of mortality; the number of events per predictive variable was ()
	for the full sample. For the simulations, at values of EPV = 2, 5,
	10, 15, 20, and 25, we randomly generated 500 samples of the 673
	patients, chosen with replacement, according to a logistic model
	derived from the full sample. Simulation results for the regression
	coefficients for each variable in each group of 500 samples were
	compared for bias, precision, and significance testing against the
	results of the model fitted to the original sample. For EPV values
	of 10 or greater, no major problems occurred. For EPV values less
	than 10, however, the regression coefficients were biased in both
	positive and negative directions; the large sample variance estimates
	from the logistic model both overestimated and underestimated the
	sample variance of the regression coeffi-cients; the 90\% confidence
	limits about the estimated values did not have proper coverage; the
	Wald statistic was conservative under the null hypothesis; and paradoxical
	associations (significance in the wrong direction) were increased.
	Although other factors (such as the total number of events, or sample
	size) may influence the validity of the logistic model, our findings
	indicate that low EPV can lead to major problems.}},
  citeulike-article-id = {2305744},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/S0895-4356(96)00236-3},
  citeulike-linkout-1 = {http://linkinghub.elsevier.com/retrieve/pii/S0895-4356(96)00236-3},
  citeulike-linkout-2 = {http://www.sciencedirect.com/science/article/B6T84-3W30X5C-8/2/f1b4e237713e6d272e85c988556f1d4e},
  doi = {10.1016/S0895-4356(96)00236-3},
  issn = {08954356},
  keywords = {logistic, model\_selection, power},
  posted-at = {2011-03-24 19:17:32},
  priority = {2},
  url = {http://dx.doi.org/10.1016/S0895-4356(96)00236-3}
}

@MANUAL{Peters2011,
  title = {{Package `ipred'}},
  author = {Andrea Peters and Torsten Hothorn},
  month = {February},
  year = {2011},
  owner = {brownlee},
  timestamp = {2011.12.12},
  url = {http://cran.r-project.org/web/packages/ipred/ipred.pdf}
}

@MANUAL{Peters2011a,
  title = {{ipred: Improved Predictors}},
  author = {Andrea Peters and Torsten Hothorn},
  year = {2011},
  owner = {brownlee},
  timestamp = {2011.12.12},
  url = {http://cran.r-project.org/web/packages/ipred/vignettes/ipred-examples.pdf}
}

@ARTICLE{Phillips1962,
  author = {Phillips, D. L.},
  title = {{A Technique for the Numerical Solution of Certain Integral Equations
	of the First Kind}},
  journal = {Journal of the ACM},
  year = {1962},
  volume = {9},
  pages = {84--97},
  number = {1},
  month = jan,
  abstract = {{An abstract is not available.}},
  address = {New York, NY, USA},
  citeulike-article-id = {2284923},
  citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=321105.321114},
  citeulike-linkout-1 = {http://dx.doi.org/10.1145/321105.321114},
  doi = {10.1145/321105.321114},
  issn = {0004-5411},
  keywords = {integral-equations, phillips\_dl, regularization},
  posted-at = {2009-02-12 11:34:39},
  priority = {2},
  publisher = {ACM},
  url = {http://dx.doi.org/10.1145/321105.321114}
}

@BOOK{Polak1971,
  title = {Computational methods in optimization: a unified approach},
  publisher = {Academic Press},
  year = {1971},
  author = {Polak, E.},
  series = {Mathematics in science and engineering},
  isbn = {9780125593502},
  lccn = {lc72134540},
  url = {http://books.google.com.au/books?id=YNaetHz0J4oC}
}

@ARTICLE{Polak1969,
  author = {Polak, E and Ribi\`ere, G},
  title = {Note sur la convergence de m\'ethodes de directions conjugu\'ees},
  journal = {Revue Francaise d'Informatique et de Reserche Op\'erationnelle},
  year = {1969},
  volume = {3},
  pages = {35--43},
  number = {1}
}

@ARTICLE{Pope1972,
  author = {Pope, P. T. and J. T. Webster},
  title = {{The use of an F-statistic in stepwise regression procedures}},
  journal = {Technometrics},
  year = {1972},
  volume = {14},
  pages = {327–-340},
  owner = {brownlee},
  timestamp = {2012.01.05}
}

@ARTICLE{Pregibon1981,
  author = {Pregibon, Daryl},
  title = {{Logistic Regression Diagnostics}},
  journal = {The Annals of Statistics},
  year = {1981},
  volume = {9},
  pages = {705--724},
  number = {4},
  abstract = {{A maximum likelihood fit of a logistic regression model (and other
	similar models) is extremely sensitive to outlying responses and
	extreme points in the design space. We develop diagnostic measures
	to aid the analyst in detecting such observations and in quantifying
	their effect on various aspects of the maximum likelihood fit. The
	elements of the fitting process which constitute the usual output
	(parameter estimates, standard errors, residuals, etc.) will be used
	for this purpose. With a properly designed computing package for
	fitting the usual maximum-likelihood model, the diagnostics are essentially
	"free for the asking." In particular, good data analysis for logistic
	regression models need not be expensive or time-consuming.}},
  citeulike-article-id = {4502392},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2240841},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2240841},
  doi = {10.2307/2240841},
  issn = {00905364},
  keywords = {diagnostics, logistic, stat\_theory},
  posted-at = {2011-01-30 15:41:30},
  priority = {0},
  publisher = {Institute of Mathematical Statistics},
  url = {http://dx.doi.org/10.2307/2240841}
}

@INBOOK{Press2007,
  chapter = {Section 10.2. Golden Section Search in One Dimension},
  pages = {492--495},
  title = {Numerical Recipes: The Art of Scientific Computing},
  publisher = {Cambridge University Press},
  year = {2007},
  author = {W. H. Press and S. A. Teukolsky and W. T. Vetterling and B. P. Flannery},
  owner = {jasonb},
  timestamp = {2011.12.26}
}

@BOOK{Quinlan1993,
  title = {C4.5: Programs for Machine Learning},
  publisher = {Morgan Kaufmann},
  year = {1993},
  author = {J. R. Quinlan},
  owner = {brownlee},
  timestamp = {2011.11.23}
}

@MANUAL{RDCT2011,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Development Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2011},
  note = {{ISBN} 3-900051-07-0},
  url = {http://www.R-project.org}
}

@MANUAL{RDCT2011a,
  title = {{Package `stats': The R Stats Package}},
  author = {{R Development Core Team}},
  organization = {R Core Team},
  year = {2011},
  owner = {jasonb},
  timestamp = {2011.12.30}
}

@BOOK{Reed1998,
  title = {Neural Smithing: Supervised Learning in Feedforward Artificial Neural
	Networks},
  publisher = {MIT Press},
  year = {1998},
  author = {Reed, R. D. and Marks, R. J.},
  address = {Cambridge, MA, USA},
  isbn = {0262181908}
}

@MANUAL{Ridgeway2007,
  title = {{Generalized Boosted Models: A guide to the gbm package}},
  author = {Greg Ridgeway},
  month = {August},
  year = {2007},
  citeulike-article-id = {7678599},
  citeulike-linkout-0 = {http://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf},
  keywords = {boosting},
  posted-at = {2010-08-19 02:48:26},
  priority = {2},
  url = {http://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf}
}

@MANUAL{Ridgeway2007a,
  title = {{Package `gbm'}},
  author = {Greg Ridgeway},
  month = {August},
  year = {2007},
  owner = {brownlee},
  timestamp = {2011.12.12},
  url = {http://cran.r-project.org/web/packages/gbm/gbm.pdf}
}

@BOOK{Russell2009,
  title = {Artificial Intelligence: A Modern Approach},
  publisher = {Prentice Hall},
  year = {2009},
  author = {S. Russell and P. Norvig},
  edition = {Third},
  owner = {jasonb},
  timestamp = {2010.01.13}
}

@ARTICLE{Shanno1970,
  author = {Shanno, D. F.},
  title = {{Conditioning of Quasi-Newton Methods for Function Minimization}},
  journal = {Mathematics of Computation},
  year = {1970},
  volume = {24},
  pages = {647--656},
  number = {111},
  month = jul,
  abstract = {{Quasi-Newton methods accelerate the steepest-descent technique for
	function minimization by using computational history to generate
	a sequence of approximations to the inverse of the Hessian matrix.
	This paper presents a class of approximating matrices as a function
	of a scalar parameter. The problem of optimal conditioning of these
	matrices under an appropriate norm as a function of the scalar parameter
	is investigated. A set of computational results verifies the superiority
	of the new methods arising from conditioning considerations to known
	methods.}},
  citeulike-article-id = {4267226},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2004840},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2004840},
  doi = {10.2307/2004840},
  issn = {00255718},
  keywords = {qbo\_nlpca2c},
  posted-at = {2009-04-03 02:28:35},
  priority = {2},
  publisher = {American Mathematical Society},
  url = {http://dx.doi.org/10.2307/2004840}
}

@BOOK{Sheather2009,
  title = {A modern approach to regression with R},
  publisher = {Springer},
  year = {2009},
  author = {Sheather, S. J.},
  series = {Springer texts in statistics},
  isbn = {9780387096070},
  lccn = {80504956},
  url = {http://books.google.com.au/books?id=zS3Jiyxqr98C}
}

@TECHREPORT{Shewchuk1994,
  author = {Shewchuk, J. R.},
  title = {{An Introduction to the Conjugate Gradient Method Without the Agonizing
	Pain}},
  institution = {School of Computer Science, Carnegie Mellon University},
  year = {1994},
  address = {Pittsburgh, PA, USA},
  citeulike-article-id = {264342},
  citeulike-linkout-0 = {http://www.cs.cmu.edu/\~{}quake-papers/painless-conjugate-gradient.pdf},
  citeulike-linkout-1 = {http://portal.acm.org/citation.cfm?id=865018},
  keywords = {optimization},
  posted-at = {2007-10-09 14:30:47},
  priority = {2},
  publisher = {Carnegie Mellon University},
  url = {http://www.cs.cmu.edu/\~{}quake-papers/painless-conjugate-gradient.pdf}
}

@TECHREPORT{Shlens2005,
  author = {Shlens, J.},
  title = {{A Tutorial on Principal Component Analysis}},
  institution = {Systems Neurobiology Laboratory, Salk Insitute for Biological Studies},
  year = {2005},
  month = {December},
  abstract = {{Principal component analysis (PCA) is a mainstay of modern data analysis
	- a black box that is widely used but poorly understood. The goal
	of this paper is to dispel the magic behind this black box. This
	tutorial focuses on building a solid intuition for how and why principal
	component analysis works; furthermore, it crystallizes this knowledge
	by deriving from simple intuitions, the mathematics behind PCA .
	This tutorial does not shy away from explaining the ideas informally,
	nor does it shy away from the mathematics. The hope is that by addressing
	both aspects, readers of all levels will be able to gain a better
	understanding of PCA as well as the when, the how and the why of
	applying this technique.}},
  citeulike-article-id = {2067469},
  citeulike-linkout-0 = {http://www.snl.salk.edu/\~{}shlens/pub/notes/pca.pdf},
  day = {10},
  posted-at = {2007-12-06 16:00:04},
  priority = {2}
}

@ARTICLE{Simon2011,
  author = {N. Simon and J. H. Friedman and T. Hastie and R. Tibshirani},
  title = {Regularization Paths for Cox's Proportional Hazards Model via Coordinate
	Descent},
  journal = {Journal of Statistical Software},
  year = {2011},
  volume = {39},
  pages = {1--13},
  number = {5},
  month = {3},
  accepted = {2010-10-02},
  bibdate = {2010-10-02},
  coden = {JSSOBK},
  day = {9},
  issn = {1548-7660},
  submitted = {2010-06-15},
  url = {http://www.jstatsoft.org/v39/i05}
}

@MISC{Smith2002,
  author = {Smith, Lindsay I.},
  title = {{A tutorial on Principal Components Analysis}},
  year = {2002},
  citeulike-article-id = {385930},
  keywords = {math},
  posted-at = {2005-11-09 18:51:31},
  priority = {0}
}

@ARTICLE{Sorenson1969,
  author = {H. W. Sorenson},
  title = {Comparison of some conjugate direction procedures for function minimization},
  journal = {Journal of The Franklin Institute},
  year = {1969},
  volume = {40},
  pages = {421--441},
  owner = {jasonb},
  timestamp = {2011.12.30}
}

@ARTICLE{Spang1962,
  author = {H. A. Spang},
  title = {{A Review of Minimization Techniques for Nonlinear Functions}},
  journal = {SIAM Review},
  year = {1962},
  volume = {4},
  pages = {343--365},
  number = {4},
  citeulike-article-id = {6904218},
  citeulike-linkout-0 = {http://www.ams.org/mathscinet-getitem?mr=26:3171},
  posted-at = {2010-03-25 06:09:33},
  priority = {2},
  url = {http://www.ams.org/mathscinet-getitem?mr=26:3171}
}

@ARTICLE{Spendley1962,
  author = {Spendley, W. and Hext, G. R. and Himsworth, F. R.},
  title = {{Sequential Application of Simplex Designs in Optimization and Evolutionary
	Operation}},
  journal = {Technometrics},
  year = {1962},
  volume = {4},
  pages = {441--461},
  citeulike-article-id = {2846890},
  keywords = {dfo, optimisation, simplex},
  posted-at = {2008-05-30 10:37:13},
  priority = {2}
}

@ARTICLE{Strobl2009,
  author = {Carolin Strobl and Torsten Hothorn and Achim Zeileis},
  title = {Party on! A New, Conditional Variable-Importance Measure for Random
	Forests Available in the party Package},
  journal = {The R Journal},
  year = {2009},
  volume = {1/2},
  pages = {14--17},
  owner = {brownlee},
  timestamp = {2011.12.09}
}

@BOOK{Sutton1998,
  title = {Reinforcement Learning: An Introduction},
  publisher = {MIT Press},
  year = {1998},
  author = {R. Sutton and A. Bardo},
  owner = {brownlee},
  timestamp = {2011.02.09}
}

@BOOK{Tamhane2000,
  title = {Statistics and data analysis: from elementary to intermediate},
  publisher = {Prentice Hall},
  year = {2000},
  author = {Tamhane, A. C. and Dunlop, D. D.},
  isbn = {9780137444267},
  lccn = {99047149},
  url = {http://books.google.com.au/books?id=IH5GAAAAYAAJ}
}

@ARTICLE{Tibshirani1996,
  author = {Tibshirani, R.},
  title = {{Regression shrinkage and selection via the lasso}},
  journal = {Journal of the Royal Statistical Society: Series B},
  year = {1996},
  volume = {58},
  pages = {267--288},
  number = {1},
  abstract = {{We propose a new method for estimation in linear models. The \&quot;lasso\&quot;
	minimizes the residual sum of squares subject to the sum of the absolute
	value of the coefficients being less than a constant. Because of
	the nature of this constraint it tends to produce some coefficients
	that are exactly zero and hence gives interpretable models. Our simulation
	studies suggest that the lasso enjoys some of the favourable properties
	of both subset selection and ridge regression. It produces interpretable
	models like subset selection and exhibits the stability of ridge
	regression. There is also an interesting relationship with recent
	work in adaptive function estimation by Donoho and Johnstone. The
	lasso idea is quite general and can be applied in a variety of statistical
	models: extensions to generalized regression models}},
  citeulike-article-id = {416068},
  citeulike-linkout-0 = {http://www.ams.org/mathscinet-getitem?mr=1379242},
  keywords = {feature-selection, lasso, regression, ridge-regression, shrinkage},
  mrnumber = {MR1379242},
  posted-at = {2008-11-13 17:23:13},
  priority = {3},
  url = {http://www.ams.org/mathscinet-getitem?mr=1379242}
}

@TECHREPORT{Tibshirani1996a,
  author = {Tibshirani, R.},
  title = {{Bias, Variance, and Prediction Error for Classification Rules}},
  institution = {Department of Statistics, University of Toronto},
  year = {1996},
  citeulike-article-id = {340511},
  keywords = {biasvariance, diplomarbeit},
  posted-at = {2005-10-04 12:14:09},
  priority = {0}
}

@ARTICLE{Tibshirani2011,
  author = {Tibshirani, R. and Bien, J. and Friedman, J. and Hastie, T. and Simon,
	N. and Taylor, J. and Tibshirani, R. J.},
  title = {Strong rules for discarding predictors in lasso-type problems},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year = {2011},
  doi = {10.1111/j.1467-9868.2011.01004.x},
  issn = {1467-9868},
  keywords = {Convex optimization, Lasso, l1-regularization, Screening, Sparsity},
  publisher = {Blackwell Publishing Ltd},
  url = {http://dx.doi.org/10.1111/j.1467-9868.2011.01004.x}
}

@ARTICLE{Tibshirani2005,
  author = {Tibshirani, R. and Saunders, M. and Rosset, S. and Zhu, J. and Knight,
	K.},
  title = {{Sparsity and smoothness via the fused lasso}},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year = {2005},
  volume = {67},
  pages = {91--108},
  number = {1},
  abstract = {{Summary.  The lasso penalizes a least squares regression by the sum
	of the absolute values (L1-norm) of the coefficients. The form of
	this penalty encourages sparse solutions (with many coefficients
	equal to 0). We propose the 'fused lasso', a generalization that
	is designed for problems with features that can be ordered in some
	meaningful way. The fused lasso penalizes the L1-norm of both the
	coefficients and their successive differences. Thus it encourages
	sparsity of the coefficients and also sparsity of their differences—i.e.
	local constancy of the coefficient profile. The fused lasso is especially
	useful when the number of features p is much greater than N, the
	sample size. The technique is also extended to the 'hinge' loss function
	that underlies the support vector classifier. We illustrate the methods
	on examples from protein mass spectroscopy and gene expression data.}},
  address = {Stanford University, USA; IBM T. J. Watson Research Center, Yorktown
	Heights, USA; University of Michigan, Ann Arbor, USA; University
	of Toronto, Canada},
  citeulike-article-id = {28022},
  citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1467-9868.2005.00490.x},
  citeulike-linkout-1 = {http://www.ingentaconnect.com/content/bpl/rssb/2005/00000067/00000001/art00007},
  citeulike-linkout-2 = {http://www3.interscience.wiley.com/cgi-bin/abstract/118712233/ABSTRACT},
  doi = {10.1111/j.1467-9868.2005.00490.x},
  issn = {1467-9868},
  keywords = {maldi, proteomics, statistics},
  posted-at = {2009-08-14 10:15:08},
  priority = {2},
  publisher = {Blackwell Publishing Ltd},
  url = {http://dx.doi.org/10.1111/j.1467-9868.2005.00490.x}
}

@INPROCEEDINGS{Tikhonov1963,
  author = {Tikhonov, A.},
  title = {{Solution of incorrectly formulated problems and the regularization
	method}},
  booktitle = {[Translated] Soviet Mathematics 4},
  year = {1963},
  volume = {4},
  pages = {1035--1038},
  citeulike-article-id = {7133170},
  keywords = {regularization},
  posted-at = {2010-05-07 01:16:41},
  priority = {2}
}

@ARTICLE{Tikhonov1943,
  author = {Tikhonov, A. N.},
  title = {{On the stability of inverse problems}},
  journal = {Dokl Akademii Nauk izvestija Poossiiskoi (Proceedings of the Russian
	Academy of Sciences)},
  year = {1943},
  volume = {39},
  pages = {195--198},
  citeulike-article-id = {6596425},
  comment = {Cross-referenced as Tikhonov1943},
  keywords = {file-import-10-01-27},
  posted-at = {2010-01-27 19:30:28},
  priority = {2}
}

@BOOK{Tikhonov1977,
  title = {{Solutions of Ill-Posed Problems}},
  publisher = {John Wiley \& Sons},
  year = {1977},
  author = {Tikhonov, A. N. and Arsenin, V. Y.},
  citeulike-article-id = {3517954},
  posted-at = {2008-11-15 02:52:39},
  priority = {2}
}

@BOOK{Torgo2009,
  title = {Data Mining with R},
  publisher = {CRC Press},
  year = {2009},
  author = {Torgo, L.},
  added-at = {2009-01-22T17:32:38.000+0100},
  biburl = {http://www.bibsonomy.org/bibtex/23a150daaf62344685dd63088a995a9c5/tmalsburg},
  description = {The main goal of this book is to introduce the reader to the use of
	R as a tool for performing data mining.},
  interhash = {427fb0ce49a4101586b5855df045fce5},
  intrahash = {3a150daaf62344685dd63088a995a9c5},
  keywords = {book datamining gnu-r introduction neuralnetworks statistics trading},
  timestamp = {2009-01-22T17:32:38.000+0100}
}

@BOOK{Walters1991,
  title = {Sequential simplex optimization: a technique for improving quality
	and productivity in research, development, and manufacturing},
  publisher = {CRC Press},
  year = {1991},
  author = {Walters, F. H.},
  series = {Chemometrics series},
  isbn = {9780849358944},
  lccn = {91014187},
  url = {http://books.google.com.au/books?id=hpxTAAAAMAAJ}
}

@BOOK{Weisberg2010,
  title = {An R Companion to Applied Regression},
  publisher = {SAGE Publications},
  year = {2010},
  author = {Weisberg, S. and Fox, J.},
  isbn = {9781412975148},
  lccn = {2010029245},
  url = {http://books.google.com.au/books?id=LMqkupSCOd4C}
}

@ARTICLE{Whittingham2006,
  author = {Whittingham, M. J. and Stephens, P. A. and Bradbury, R. B. and Freckleton,
	R. P.},
  title = {{Why do we still use stepwise modelling in ecology and behaviour?}},
  journal = {Journal of Animal Ecology},
  year = {2006},
  volume = {75},
  pages = {1182--1189},
  number = {5},
  month = sep,
  abstract = {{Summary * 1The biases and shortcomings of stepwise multiple regression
	are well established within the statistical literature. However,
	an examination of papers published in 2004 by three leading ecological
	and behavioural journals suggested that the use of this technique
	remains widespread: of 65 papers in which a multiple regression approach
	was used, 57\% of studies used a stepwise procedure. * 2The principal
	drawbacks of stepwise multiple regression include bias in parameter
	estimation, inconsistencies among model selection algorithms, an
	inherent (but often overlooked) problem of multiple hypothesis testing,
	and an inappropriate focus or reliance on a single best model. We
	discuss each of these issues with examples. * 3We use a worked example
	of data on yellowhammer distribution collected over 4 years to highlight
	the pitfalls of stepwise regression. We show that stepwise regression
	allows models containing significant predictors to be obtained from
	each year's data. In spite of the significance of the selected models,
	they vary substantially between years and suggest patterns that are
	at odds with those determined by analysing the full, 4-year data
	set. * 4An information theoretic (IT) analysis of the yellowhammer
	data set illustrates why the varying outcomes of stepwise analyses
	arise. In particular, the IT approach identifies large numbers of
	competing models that could describe the data equally well, showing
	that no one model should be relied upon for inference.}},
  address = {Department of Mathematics, University of Bristol, University Walk,
	Bristol, BS8 1TW, UK; ; Royal Society for the Protection of Birds,
	The Lodge, Sandy, Bedfordshire, SG19 2DL, UK; and ; Department of
	Animal and Plant Sciences, University of Sheffield, Sheffield S10
	2TN, UK},
  citeulike-article-id = {786972},
  citeulike-linkout-0 = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.1365-2656.2006.01141.x},
  citeulike-linkout-1 = {http://dx.doi.org/10.1111/j.1365-2656.2006.01141.x},
  citeulike-linkout-2 = {http://www.ingentaconnect.com/content/bsc/janim/2006/00000075/00000005/art00016},
  citeulike-linkout-3 = {http://view.ncbi.nlm.nih.gov/pubmed/16922854},
  citeulike-linkout-4 = {http://www.hubmed.org/display.cgi?uids=16922854},
  citeulike-linkout-5 = {http://www3.interscience.wiley.com/cgi-bin/abstract/118727122/ABSTRACT},
  doi = {10.1111/j.1365-2656.2006.01141.x},
  issn = {0021-8790},
  keywords = {plant\_ecology, statistics},
  pmid = {16922854},
  posted-at = {2009-03-23 10:12:31},
  priority = {2},
  publisher = {Blackwell Publishing Ltd},
  url = {http://dx.doi.org/10.1111/j.1365-2656.2006.01141.x}
}

@ARTICLE{Wilkinson1979,
  author = {L. Wilkinson},
  title = {Tests of signiﬁcance in stepwise regression},
  journal = {Psychological Bulletin},
  year = {1979},
  volume = {86},
  pages = {168--174},
  number = {1},
  owner = {brownlee},
  timestamp = {2012.01.05}
}

@BOOK{Witten2011,
  title = {Data Mining: Practical Machine Learning Tools and Techniques},
  publisher = {Morgan Kaufmann Publishers},
  year = {2011},
  author = {I. H. Witten and E. Frank and M. A. Hall},
  edition = {Third},
  owner = {jasonb},
  timestamp = {2010.01.09}
}

@ARTICLE{Wu2008,
  author = {T. T. Wu and K. Lange},
  title = {Coordinate descent algorithms for lasso penalized regression},
  journal = {Annals of Applied Statistics,},
  year = {2008},
  volume = {2},
  pages = {224--244},
  number = {1},
  owner = {jasonb},
  timestamp = {2012.02.11}
}

@ARTICLE{Yuan2006,
  author = {Yuan, M. and Lin, Y.},
  title = {{Model selection and estimation in regression with grouped variables}},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year = {2006},
  volume = {68},
  pages = {49--67},
  number = {1},
  month = feb,
  abstract = {{Summary.  We consider the problem of selecting grouped variables
	(factors) for accurate prediction in regression. Such a problem arises
	naturally in many practical situations with the multifactor analysis-of-variance
	problem as the most important and well-known example. Instead of
	selecting factors by stepwise backward elimination, we focus on the
	accuracy of estimation and consider extensions of the lasso, the
	LARS algorithm and the non-negative garrotte for factor selection.
	The lasso, the LARS algorithm and the non-negative garrotte are recently
	proposed regression methods that can be used to select individual
	variables. We study and propose efficient algorithms for the extensions
	of these methods for factor selection and show that these extensions
	give superior performance to the traditional stepwise backward elimination
	method in factor selection problems. We study the similarities and
	the differences between these methods. Simulations and real examples
	are used to illustrate the methods.}},
  address = {Georgia Institute of Technology, Atlanta, USA; University of WisconsinMadison,
	USA},
  citeulike-article-id = {448082},
  citeulike-linkout-0 = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.1467-9868.2005.00532.x},
  citeulike-linkout-1 = {http://dx.doi.org/10.1111/j.1467-9868.2005.00532.x},
  citeulike-linkout-2 = {http://www.ingentaconnect.com/content/bpl/rssb/2006/00000068/00000001/art00004},
  citeulike-linkout-3 = {http://www3.interscience.wiley.com/cgi-bin/abstract/118566968/ABSTRACT},
  doi = {10.1111/j.1467-9868.2005.00532.x},
  issn = {1369-7412},
  keywords = {group-lasso, lasso},
  posted-at = {2011-02-09 13:23:12},
  priority = {2},
  publisher = {Blackwell Publishing Ltd},
  url = {http://dx.doi.org/10.1111/j.1467-9868.2005.00532.x}
}

@ARTICLE{Zhu1997,
  author = {Zhu, C. and Byrd, R. H. and Lu, P. and Nocedal, J.},
  title = {{Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained
	optimization}},
  journal = {ACM Transactions on Mathematical Software},
  year = {1997},
  volume = {23},
  pages = {550--560},
  month = {December},
  acmid = {279236},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/279232.279236},
  issn = {0098-3500},
  issue = {4},
  issue_date = {Dec. 1997},
  keywords = {large-scale optimization, limited-memory method, nonlinear optimization,
	variable metric method},
  numpages = {11},
  publisher = {ACM},
  url = {http://doi.acm.org/10.1145/279232.279236}
}

@TECHREPORT{Zhu2008,
  author = {X. Zhu},
  title = {Semi-Supervised Learning Literature Survey},
  institution = {University of Wisconsin, Madison},
  year = {2008},
  number = {TR1530},
  owner = {brownlee},
  timestamp = {2011.02.10}
}

@BOOK{Zhu2009,
  title = {Introduction to Semi-Supervised Learning},
  publisher = {Morgan and Claypool Publishers},
  year = {2009},
  author = {X. Zhu and A. B. Goldberg},
  owner = {brownlee},
  timestamp = {2011.02.10}
}

@ARTICLE{Zou2005,
  author = {H. Zou and T. Hastie},
  title = {Regularization and variable selection via the elastic net},
  journal = {Journal Of The Royal Statistical Society Series B},
  year = {2005},
  volume = {67},
  pages = {301-320},
  number = {2},
  abstract = {This study investigates the impact of the current financial crisis
	on Canada's potential GDP growth. Using a simple accounting framework
	to decompose trend GDP growth into changes in capital, labor services
	and total factor productivity, we find a sizeable drop in Canadian
	potential growth in the short term. The estimated decline of about
	1 percentage point originates from a sharply decelerating capital
	stock accumulation (as investment has dropped steeply) and a rising
	long-term unemployment rate (which would raise equilibrium unemployment
	rates). However, over the medium term, we expect Canada's potential
	GDP growth to gradually rise to around 2 percent, below the pre-crisis
	growth rate, mostly reflecting the effects of population aging and
	a secular decline in average working hours.},
  url = {http://ideas.repec.org/a/bla/jorssb/v67y2005i2p301-320.html}
}

@BOOK{Chapelle2010,
  title = {Semi-Supervised Learning},
  publisher = {The MIT Press},
  year = {2010},
  editor = {O. Chapelle and B. Sch\"olkopf and A. Zien},
  owner = {brownlee},
  timestamp = {2011.02.10}
}

