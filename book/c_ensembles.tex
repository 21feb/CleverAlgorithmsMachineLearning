% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is a chapter

\renewcommand{\bibsection}{\subsection{\bibname}}
\begin{bibunit}

\chapter{Ensembles}
\label{ch:ensembles}
\index{Ensembles}

\section{Overview}
This chapter describes Ensembles methods.


% Strategy: Problem solving plan
% The strategy is an abstract description of the computational model. The strategy describes the information processing actions a technique shall take in order to achieve an objective. The strategy provides a logical separation between a computational realization (procedure) and a analogous system (metaphor). A given problem solving strategy may be realized as one of a number specific algorithms or problem solving systems. The strategy description is textual using information processing and algorithmic terminology.
\subsection{Strategy}
% What is the information processing objective of a technique?
% What is a techniques plan of action?


efficient - weak learners are easier to train than finding the best model
difficulty in selecting a model
Weak learner and basis function are synonyms.
There is now theory on why ensembles work (performing a Monte Carlo simulation for an integration problem).
Best results from combining low bias, high-variance learners

strategies
- vary the models (functions)
- vary the data

compmenent of model selection - combines measures of fit in cross validation
ensembles combine fitted values (predictions)


BOOSTING
prepare each tree on a weighted sample
fix the mistakes made before
more clever version of bagging
build tree, assign weights based on mistakes, build next tree, so on
weighted averages of observations when splitting

gradient descent of a loss function... modern understanding - gradient boosting

control the size of the trees - quite small - faster

train beyond perfect train data - test error gets better!
makes the problem harder and harder - but the error does start increasing again

MODERN:
building an expanding basis function - stepwise additive modelling - does it iteratively - forward stage wise learning
like basis pursuit is of the same form - matching pursuit




% Heuristics: Usage guidelines
% The heuristics element describe the commonsense, best practice, and demonstrated rules for applying and configuring a parameterized algorithm. The heuristics relate to the technical details of the techniques procedure and data structures for general classes of application (neither specific implementations not specific problem instances). The heuristics are described textually, such as a series of guidelines in a bullet-point structure.
\subsection{Heuristics}
% What are the suggested configurations for a technique?
% What are the guidelines for the application of a technique to a problem instance?


\begin{itemize}
	\item Many weak learners can out-perform a single strong learner.
	\item The diversity of the ensemble set is important property of success. Don't want all models to tell the same story - want different points of view.
	\item Model must be better than random, models must be independent (data, modeled function, etc).
\end{itemize}


% References: Deeper understanding
% The references element description includes a listing of both primary sources of information about the technique as well as useful introductory sources for novices to gain a deeper understanding of the theory and application of the technique. The description consists of hand-selected reference material including books, peer reviewed conference papers, journal articles, and potentially websites. A bullet-pointed structure is suggested.
\subsection{References}
% What are the primary sources for a technique?
% What are the suggested reference sources for learning more about a technique?

% primary sources
\subsubsection{Primary Sources}



% more info
\subsubsection{More Information}

Early overview that was cited a lot \cite{Dietterich2000}.

\putbib
\end{bibunit}


\newpage\begin{bibunit}\input{a_ensembles/bagging}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_ensembles/adaboost}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_ensembles/gradient_boosting}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_ensembles/random_forest}\putbib\end{bibunit}

