% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2011 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is a chapter

\renewcommand{\bibsection}{\subsection{\bibname}}
\begin{bibunit}

\chapter{Optimization}
\label{ch:optimization}
\index{Optimization}

\section{Overview}
This chapter describes unconstrained Optimization methods that common in the field of Machine Learning.

% Types of Optimization Algorithms
\subsection{Taxonomy}
\index{Direct Search}
\index{Stochastic Direct Search}
\index{Convex Optimization}
\index{Nonlinear Optimization}
The field of function optimization is mature, traditionally referred to as Mathematical Programming. Optimization forms the core of most Machine Learning methods, and many explicitly use a classical optimization method. For the purposes of our understanding of optimization in the context of Machine Learning, we can consider the following types of optimization algorithms:

\begin{itemize}
	\item \textbf{Direct Search}: These are methods that only sample the function to decide how to progress the search. These are generally inefficient compared to derivative based approaches, but have the advantage of working on functions where a derivative cannot be computed or cannot be computed easily, such as noisy or discontinuous functions. Any gradient information is inferred from direct samples. The name `Direct Search' was proposed by Hooke and Jeeves \cite{Hooke1961} and a modern review of such methods can be seen in Lewis et~al. \cite{Lewis2000}. Examples of Direct Search algorithms include Line Search methods, Pattern Search, Rosenbrock's Method, Nelder-Mead Search, Powell's Conjugate Direction Method.
	\item \textbf{First-order Derivative}: Examples include Gradient Descent, Steepest Descent, and Conjugate Gradient Descent.
	\item \textbf{Second-order Derivative}: Examples include Newton's Method, Gauss-Newton Method, Levenberg-Marquardt Method, Quasi-Newton Methods, BFGS and LM-BFGS.
	\item \textbf{Stochastic Direct Search}: These are methods that are stochastic processes that sample the function. Examples include Simulated Annealing, Genetic Algorithms, and Particle Swarm Optimization.
\end{itemize}

% Nomenclature
\subsection{Nomenclature}
% conceputlaization
Function optimization is commonly conceptualized geometrically, where the parameters of the function define an $n$-dimensional search space and the role of the algorithm is to locate the desired point within that search space.
% optima
A problem may be referred to as an objective function, cost function, utility function, or loss function and it may be minimization or maximization. One may talk about the objective of the function in the abstract as a function extrema or function optima. 
% shape
A function have one or more optima, and the shape of the functions response surface (the result of the function given parameter values) is referred to as being unimodal or multimodal respectively. The single optima of a unimodal function means that the local optima is the global optima. High-nonlinear and multimodal problems can have many optima, meaning that some optimization methods can get caught in local optima and fail to fund the global optima.
% convex
Many Machine Learning methods have been defined with the optimization of a convex function as a the core problem. For our purposes, a convex function is unimodal and is a type of function that looks like a bowl or basin shape when in two-dimensions.

% references
\subsection{References}
% general
There are many texts on modern optimization methods. Some good general reference texts include Boyd and Vandenberghe  who provide a comprehensive introduction into the field of convex function optimization \cite{Boyd2004}, and Griva et~al. who provide a broader walk of the field of linear and non-linear optimization \cite{Griva2009}.
% for machine learning
Reed et~al. provide an excellent overview of modern optimization methods in the context of their adaptation for use in Artificial Neural Networks \cite{Reed1998} (Chapter 10).
% R 
Braun et~al. provide a gentle introduction to optimization algorithms in R with some worked examples \cite{Braun2007} (Chapter 7).

\putbib
\end{bibunit}

\newpage\begin{bibunit}\input{a_optimization/golden_section_search}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_optimization/gradient_descent}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_optimization/nelder_mead}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_optimization/conjugate_gradient}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_optimization/bfgs}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_optimization/lbfgs}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_optimization/irls}\putbib\end{bibunit}

