% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2011 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is a chapter

\renewcommand{\bibsection}{\subsection{\bibname}}
\begin{bibunit}

\chapter{Optimization}
\label{ch:optimization}
\index{Optimization}
\index{Function Optimization}

\section{Overview}
This chapter describes unconstrained Optimization methods that common in the field of Machine Learning.

% Types of Optimization Algorithms
\subsection{Taxonomy}
index{Function Optimization!Taxonomy}
The field of function optimization is mature, traditionally referred to as Mathematical Programming. Optimization forms the core of most Machine Learning methods, and many explicitly use a classical optimization method. For the purposes of our understanding of optimization in the context of Machine Learning, we can consider the following types of optimization algorithms:

\index{Direct Search}
\index{Stochastic Direct Search}
\index{Convex Optimization}
\index{Nonlinear Optimization}
\begin{itemize}
	\index{Line Search}
	\index{Pattern Search}
	\index{Rosenbrock's Method}
	\index{Nelder-Mead Search}
	\index{Powell's Conjugate Direction Method}
	\item \textbf{Direct Search}: These are methods that only sample the function to decide how to progress the search. These are generally inefficient compared to derivative based approaches, but have the advantage of working on functions where a derivative cannot be computed or cannot be computed easily, such as noisy or discontinuous functions. Any gradient information is inferred from direct samples. The name `Direct Search' was proposed by Hooke and Jeeves \cite{Hooke1961} and a modern review of such methods can be seen in Lewis et~al. \cite{Lewis2000}. Examples of Direct Search algorithms include Line Search methods, Pattern Search, Rosenbrock's Method, Nelder-Mead Search, and Powell's Conjugate Direction Method.

	\index{Simulated Annealing}
	\index{Genetic Algorithm}
	\index{Particle Swarm Optimization}
	\item \textbf{Stochastic Direct Search}: These are methods that are stochastic processes that sample the objective function in a direct manner. Examples include Simulated Annealing, Genetic Algorithms, and Particle Swarm Optimization.

	\index{Gradient Descent}
	\index{Steepest Descent}
	\index{Conjugate Gradient Descent}
	\item \textbf{First-order Derivative}: These are methods that require the calculation of the first partial derivative (function gradient) of the objective function that is being optimized. For those problems where a gradient can be computed directly, these methods are generally more efficient (converge faster) than Direct Search methods. Examples include Gradient Descent, Steepest Descent, and Conjugate Gradient Descent.

	\index{Newton's Method}
	\index{Newton-Raphson Method}
	\index{Gauss-Newton Method}
	\index{Levenberg-Marquardt Method}
	\index{quasi-Newton Methods}
	\index{BFGS}
	\index{DFP Method}
	\index{Broyden's Method}
	\index{SR1 Method}
	\item \textbf{Second-order Derivative}: These are methods that require the calculation of a Hessian matrix of second partial derivatives (or an approximation thereof) in addition to the first-order gradient. Some second-order methods also require the storage of a matrix which must be maintained during the search procedure. Compared to first-order methods, these methods have a grater computational and/or memory expense although are generally more efficient (converge faster) given the grater amount of information available about the function. Examples include Newton's Method (Newton-Raphson Method), Gauss-Newton Method, Levenberg-Marquardt Method, and quasi-Newton Methods (Hessian approximation methods) such as BFGS, DFP Method, Broyden's method and the SR1 Method.
\end{itemize}

% Nomenclature
\subsection{Nomenclature}
index{Function Optimization!Nomenclature}
% conceputlaization
Function optimization is commonly conceptualized geometrically, where the parameters of the function define an $n$-dimensional search space and the role of the algorithm is to locate the desired point within that search space.

% optima
A problem may be referred to as an objective function, cost function, utility function, or loss function and it may be minimization or maximization. One may talk about the objective of the function in the abstract as a function extremum or function optimum. 
% shape
A function have one or more optima, and the shape of the functions response surface (the result of the function given parameter values) is referred to as being unimodal or multimodal respectively. The single optimum of a unimodal function means that the local optimum is the global optimum. Highly-nonlinear and multimodal problems can have many optima, meaning that some optimization methods can get caught in a local optimum and fail to fund the global optimum.

% convex
Many Machine Learning methods have been defined with the optimization of a convex function as the core problem. For our purposes, a convex function is unimodal and is a type of function that looks like a bowl or basin shape when in two-dimensions.

% references
\subsection{Further Reading}
\index{Function Optimization!Further Reading}
% general
There are many texts on modern optimization methods. Some good general reference texts include Boyd and Vandenberghe  who provide a comprehensive introduction into the field of convex function optimization \cite{Boyd2004}, and Griva et~al. who provide a broader walk of the field of linear and non-linear optimization \cite{Griva2009}. Nocedal and Wright also provide a thorough text on numerical optimization methods \cite{Nocedal1999}.
% for machine learning
Reed et~al. provide an excellent overview of modern optimization methods in the context of their adaptation for use in Artificial Neural Networks \cite{Reed1998} (Chapter 10).
% R 
Braun et~al. provide a gentle introduction to optimization algorithms in R with some worked examples \cite{Braun2007} (Chapter 7).

\putbib
\end{bibunit}

\newpage\begin{bibunit}\input{a_optimization/golden_section_search}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_optimization/nelder_mead}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_optimization/gradient_descent}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_optimization/conjugate_gradient}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_optimization/bfgs}\putbib\end{bibunit}
