% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is a chapter

\renewcommand{\bibsection}{\subsection{\bibname}}
\begin{bibunit}

\chapter{Regression}
\label{ch:regression}
\index{Regression}

\section{Overview}
This chapter describes Regression methods that common in the field of Machine Learning.

% Types of Regression Algorithms
\subsection{Taxonomy}
Regression is concerned with modeling the relationships between variables. In Machine Learning such methods are commonly used for predicting real and categorical variables. There are many frameworks used to understand regression models, this section summarizes some of them to provide a context for the regression methods described in this chapter.

\textbf{Linear Regression}: These are models where the relationship between dependent and independent variables is modeled using a linear function (first-degree polynomial) also known as Linear Regression. We find the set of coefficients that minimize the empirical loss, most commonly the squared error over all observations (referred to as $L_2$) assuming normally distributed noise. 

Extensions to Linear Regression are named after the feature they provide, for example \textbf{Multivariate Linear Regression} (also known as Multiple Linear Regression) is an extension from one independent variable (Univariate Linear Regression) to multiple independent variables.

Methods for estimating the coefficients...

% Non-Linear Regression ?????

% Examples are named after the methods for finding the coefficients, such as Ordinary Least Squares (OLR).

% , Generalized Least Squares (GLS), Iteratively Reweighted Least Squares (IRLS), Ridge Regression (RR), and Least-Angle Regression (LAR).

\subsubsection{Generalized Linear Model}
Generalized Linear Models (GLM) is a framework proposed by Nelder and Wedderburn that generalizes Linear Regression to a broader class of maximum likelihood estimators \cite{Nelder1972}. A model under this framework is characterised by:

\begin{itemize}
	\item A conditional probability distribution for the dependent variable (\texttt{y}) that is assumed to be in the exponential family (such as Normal, Binomial, Poisson, Gamma).
	\item A predictor that linearly combines the independent variables using coefficients (regressors). The predictor is commonly fit by the maximum likelihood estimates using Iteratively Reweighted Linear Regression (IRLR) optimization procedure.
	\item A smooth and invertible link function that associates the predictors value with the mean of the probability distribution function, transforming the response from the predictor into an expected value of the data (output).
\end{itemize}

 GLMs models are most commonly fit by the method of maximum likelihood. 
The framework unified Linear Regression (normal distribution), Logistic Regression (binomial distribution), and Poisson Regression (poisson distribution). The Generalized Linear Model framework was proposed by Nelder and Wedderburn \cite{Nelder1972}. McCullagh and Nelder provide an in depth presentation in their seminal text \cite{McCullagh1989}.

extension of the Gauss-Markov theorem in what became known as Geeralized Least Squares (GLS) \cite{Aitken1935}.

concise history of GLMs \cite{McCulloch2000}


Polynomial regression - adding new variables


Nonlinear Regression
Nonparametric Regression - Generalized Nonparametric Regression

Locally Weighted Regression
Principle Component Regression
Stepwise Regression
Ridge Regression


% Nomenclature
\subsection{Nomenclature}

bias = intercept (beta 0)
regression line
dependent variable (Y)
independent variable (X)
coefficients (beta)
best fit
residual
regressors - coefficient


% references
\subsection{Further Reading}


Regression with R - book that covers linear and generalized linear models with all examples in R \cite{Weisberg2010}

another good book on regression with R \cite{Sheather2009}



\putbib
\end{bibunit}

\newpage\begin{bibunit}\input{a_regression/ordinary_least_squares_regression}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/logistic_regression}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/stepwise_linear_regression}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/multivariate_adaptive_regression_splines}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/locally_weighted_regression}\putbib\end{bibunit}

