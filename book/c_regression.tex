% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is a chapter

\renewcommand{\bibsection}{\subsection{\bibname}}
\begin{bibunit}

\chapter{Regression}
\label{ch:regression}
\index{Regression}

\section{Overview}
This chapter describes Regression methods that are common in the field of Machine Learning.

% Types of Regression Algorithms
\subsection{Taxonomy}
\index{Regression!Taxonomy}
Regression is concerned with modeling the relationships between variables, commonly as a line, surface or hyper-surface of best fit. In Machine Learning such methods are commonly used for predicting real and categorical variables. There are many frameworks used to understand regression models, this section summarizes some of them to provide a context for the regression methods described in this chapter.

% Linear Regression
\index{Linear Regression}
\subsubsection{Linear Regression}
These are models where the relationship between dependent and independent variables is modeled using a linear function (first-order polynomial) also known as Linear Regression. Methods find the set of coefficients that minimize the empirical loss, most commonly the squared error over all observations assuming normally distributed noise.

\index{Multivariate Linear Regression}
\index{Multiple Linear Regression}
Extensions to Linear Regression are named after the feature they provide, for example Multivariate Linear Regression (also known as Multiple Linear Regression) is an extension from one independent variable (Univariate) Linear Regression to support multiple independent variables.

Numerical methods are used fit the models (estimating the coefficients) commonly by minimizing a cost or loss function, such as using first- or second-order derivative optimization algorithm to minimize the Sum of Squared Residual (SSR) error.

% Generalized Linear Model
\index{GLM}
\index{Generalized Linear Model}
\subsubsection{Generalized Linear Model}
Generalized Linear Models (GLM) are a framework that generalizes Linear Regression to a broader class of maximum likelihood estimators. A model under this framework is characterized by the following:

\index{Iteratively Reweighted Linear Regression}
\begin{itemize}
	\item A conditional probability distribution for the dependent variable (\texttt{y}) that is assumed to be in the exponential family (such as Normal, Binomial, Poisson, Gamma).
	\item A predictor that linearly combines the independent variables using coefficients (regressors). The predictor is commonly fit by the maximum likelihood estimates using the Iteratively Reweighted Linear Regression (IRLR) optimization procedure.
	\item A smooth and invertible link function that associates the predictors value with the mean of the probability distribution function, transforming the response from the predictor into an expected value of the data (output).
\end{itemize}

\index{Linear Regression}
\index{Logistic Regression}
\index{Poisson Regression}
Examples of generalized linear models include Linear Regression (normal distribution), Logistic Regression (binomial distribution), and Poisson Regression (poisson distribution). 

% Generalized Additive Model
\index{Generalized Additive Model}
\index{GAM}
\index{Non-Linear Regression}
\index{Multivariate Adaptive Regression Splines}
\index{Locally Estimated Scatterplot Smoothing}
\subsubsection{Generalized Additive Model}
Generalized Additive Model (GAM) are a framework that updates the Generalized Linear Models to support Additive Models or Non-Linear Regression (a fit achieved using a non-parametric method).
% exaqmples
Examples of Non-Linear Regression methods include Multivariate Adaptive Regression Splines (MARS) and Locally Estimated Scatterplot Smoothing (LOESS).


% Nomenclature
\subsection{Nomenclature}
\index{Regression!Nomenclature}
Discussions on regression use terms from statistics and probability. This section touches on a few common terms used when discussing regression methods.

Given a plot of data in two-dimensions (\texttt{x} and \texttt{y}), imagine a line that best describes the relationship between \texttt{x} and \texttt{y}. This line is called the regression line or the line of best fit. The point where the line touches the \texttt{y}-axis is called the \texttt{y}-intercept or in Machine Learning, the bias. The equation that describes the relationship between the variables is called the model. The model is prepared by a regression algorithm, where the coefficients that are found by the method are called the regressors. The regression equation answers the question, given a value \texttt{x} (independent variable), what is the value \texttt{y} (dependent variable).

% references
\subsection{Further Reading}
\index{Regression!Further Reading}
% general regression analysis
There are many excellent texts devoted to regression analysis.
Tamhane and Dunlop provide an excellent introduction to the field in their text book \cite{Tamhane2000}, as does Montgomery \cite{Montgomery2001}.
% regression in R
Weisberg and Fox provide a good book for getting started with regression, covering linear and generalized linear models with all examples in R \cite{Weisberg2010}. Sheather also provides an good introduction to regression using the R platform \cite{Sheather2009}.

% GLM
The Generalized Linear Models framework was proposed by Nelder and Wedderburn \cite{Nelder1972}. McCullagh and Nelder provide an in depth presentation in their seminal text \cite{McCullagh1989}. McCulloch provides a concise history of Generalized Linear Models \cite{McCulloch2000}.
% GAM
Hastie and Tibshirani described the Generalized Additive Models framework \cite{Hastie1986}, who later published an extensive book on the topic \cite{Hastie1990}.

\putbib
\end{bibunit}

\newpage\begin{bibunit}\input{a_regression/ordinary_least_squares_regression}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/logistic_regression}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/stepwise_regression}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/multivariate_adaptive_regression_splines}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/locally_estimated_scatterplot_smoothing}\putbib\end{bibunit}

