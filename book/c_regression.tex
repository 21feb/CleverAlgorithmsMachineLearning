% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is a chapter

\renewcommand{\bibsection}{\subsection{\bibname}}
\begin{bibunit}

\chapter{Regression}
\label{ch:regression}
\index{Regression}

\section{Overview}
This chapter describes Regression methods that common in the field of Machine Learning.

% Types of Regression Algorithms
\subsection{Taxonomy}
Regression is concerned with modeling the relationships between variables, commonly as a line, surface or hyper-surface of best fit. In Machine Learning such methods are commonly used for predicting real and categorical variables. There are many frameworks used to understand regression models, this section summarizes some of them to provide a context for the regression methods described in this chapter.

% Linear Regression
\subsubsection{Linear Regression}
These are models where the relationship between dependent and independent variables is modeled using a linear function (first-order polynomial) also known as Linear Regression. Methods find the set of coefficients that minimize the empirical loss, most commonly the squared error over all observations (referred to as $L_2$) assuming normally distributed noise.

Extensions to Linear Regression are named after the feature they provide, for example \textbf{Multivariate Linear Regression} (also known as Multiple Linear Regression) is an extension from one independent variable (Univariate) Linear Regression to support multiple independent variables.

Numerical methods are used fit the models (estimating the coefficients) commonly by minimising a cost or loss function, such as using first- or second-order derivative optimization algorithm to minimize the Sum of Squared Residual (SSR) error.

% Generalized Linear Model
\subsubsection{Generalized Linear Model}
Generalized Linear Models (GLM) are a framework that generalizes Linear Regression to a broader class of maximum likelihood estimators. A model under this framework is characterised by the following characteristics:

\begin{itemize}
	\item A conditional probability distribution for the dependent variable (\texttt{y}) that is assumed to be in the exponential family (such as Normal, Binomial, Poisson, Gamma).
	\item A predictor that linearly combines the independent variables using coefficients (regressors). The predictor is commonly fit by the maximum likelihood estimates using Iteratively Reweighted Linear Regression (IRLR) optimization procedure.
	\item A smooth and invertible link function that associates the predictors value with the mean of the probability distribution function, transforming the response from the predictor into an expected value of the data (output).
\end{itemize}


The framework unified Linear Regression (normal distribution), Logistic Regression (binomial distribution), and Poisson Regression (poisson distribution). 

Polynomial regression - adding new variables


Nonlinear Regression
Nonparametric Regression - Generalized Nonparametric Regression

Locally Weighted Regression
Principle Component Regression
Stepwise Regression
Ridge Regression
Robust Regression

% Generalized Additive Model
\subsubsection{Generalized Additive Model}
generalization of additive models
basically non-parametric regression methods

MARS
Linear Weighted Regression or LOESS

paper: \cite{Hastie1986}
book: \cite{Hastie1990}



% Nomenclature
\subsection{Nomenclature}

bias = intercept (beta 0)
regression line
dependent variable (Y)
independent variable (X)
coefficients (beta)
best fit
residual
regressors - coefficient


% references
\subsection{Further Reading}


Regression with R - book that covers linear and generalized linear models with all examples in R \cite{Weisberg2010}

another good book on regression with R \cite{Sheather2009}


The Generalized Linear Model framework was proposed by Nelder and Wedderburn \cite{Nelder1972}. McCullagh and Nelder provide an in depth presentation in their seminal text \cite{McCullagh1989}.

extension of the Gauss-Markov theorem in what became known as Generalized Least Squares (GLS) \cite{Aitken1935}.

concise history of GLMs \cite{McCulloch2000}


\putbib
\end{bibunit}

\newpage\begin{bibunit}\input{a_regression/ordinary_least_squares_regression}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/logistic_regression}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/stepwise_regression}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/multivariate_adaptive_regression_splines}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regression/locally_estimated_scatterplot_smoothing}\putbib\end{bibunit}

