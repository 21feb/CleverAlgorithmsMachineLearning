% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is a chapter

\renewcommand{\bibsection}{\subsection{\bibname}}
\begin{bibunit}

\chapter{Regularization}
\label{ch:regularization}
\index{Regularization}

\section{Overview}
This chapter describes Regularization.

% what is regularization?
\subsection{Description}
\index{Regularization!Description}
% what is it
Regularization is generally a method for fine tuning of the bias-variance trade-off of a model in the context of a training dataset by penalizing model complexity.
% how does it generally work
A common example in linear models is to modify the fit equation and/or loss function to include a penalty for the number or absolute magnitude of summed model coefficients. Such modifications would then favoring models with fewer coefficients or lower a lower absolute magnitude of summed coefficients. The effect is simpler models and the removal of parameters as coefficient values move toward zero. A validation dataset (separate from the training dataset) or cross validation is commonly used to assess the predictive capability of the model during the regularization process.

% abstraction
In this regard, regularization may be conceptualized as feature selection or model selection that favors fewer features or less complex models. The reduction of complexity or shrinking of the model can reduce variance in the model and improve the models ability to generalize (address the model over-fitting the training dataset). 
% when to use
Regularization is useful when the risk of overfitting is high, such as when you have small amount of training data, few observations of a given type, or when you have more features than you do observations.

% examples of methods
\subsection{Taxonomy}
\index{Regularization!Taxonomy}
A useful way to conceptualize the study of Regularization methods is to consider the type of penalty that is added to the cost function.

\index{LASSO}
\index{Basis Pursuit}
\index{Group LASSO}
\index{Blockwise Sparse Regression}
\index{Fused LASSO}
\index{Graphical LASSO}
\index{Ridge Regression}
\index{Elastic Net}
\index{Smoothly Clipped Absolute Deviation}
\index{SCAD}
\index{$L_1$-norm Penalty}
\index{$L_2$-norm Penalty}
\index{Bridge Regression}
\index{Non-negative Garrote}
\index{Stepwise Regression}
\index{Subset Selection}
\begin{description}
	\item[$L_1$-norm Penalty] This is the sum of the absolute values in a regression model or the $L_1$-norm of the coefficient vector. Examples include the Dantzig selector, and the LASSO (a.k.a Basis Pursuit) and extensions to the LASSO such as Group LASSO, Blockwise Sparse Regression (BSR), Fused LASSO, Graphical LASSO.
	\item[$L_2$-norm Penalty] This is the squared sum of the absolute values in a regression model or the $L_2$-norm of the coefficient vector. Examples include Ridge Regression.
	\item[Hybrid of $L_1$-norm and $L_2$-norm Penalty] These are methods that makes use of both types of penalty or combine them in some way. Examples include the Elastic Net and Smoothly Clipped Absolute Deviation (SCAD) penalty.
	\item[$L_q$-norm Penalty] These are a generalization of the $L_1$ and $L_2$ norm penalties. Examples include Bridge Regression.
	\item[Other] These are regularization methods that don't fit into the chosen taxonomy. Examples include the Non-negative Garrote. Other methods related to regularization that are commonly directly compared in simulation studies include Stepwise Regression and Subset Selection.
\end{description}

Methods such as the LASSO and Elastic Net pose difficult function optimization problems. Much research goes into the development of efficient algorithmic solutions to these objective functions. Examples include Least Angle Regression (LARS) and Cyclical Coordinate Descent.

You can regularize many different model types. For example, it is common to prepend ``Regularized'' to the names models that make use of a regularization method such as Regularized Linear Regression and Regularized Support Vector Machine. It is also common for regularization methods to be described and analyzed in the context of simple regression models such as Linear Regression.

% further reading
\subsection{Further Reading}
\index{Regularization!Further Reading}
Regularization as a subject of study in its own right is a relatively recent development. There are few if any texts dedicated to it. Some Machine Learning texts provide a treatment of the subject in the context of modifications to regression models.

Nevertheless, Hastie et~al.\ provide a detailed treatment of regularization methods (\cite{Hastie2009}, Chapter 3) and also go on to describe more sophisticated methods (\cite{Hastie2009}, Chapter 5).

\putbib
\end{bibunit}

\newpage\begin{bibunit}\input{a_regularization/ridge_regression}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regularization/lasso}\putbib\end{bibunit}
\newpage\begin{bibunit}\input{a_regularization/elastic_net}\putbib\end{bibunit}
