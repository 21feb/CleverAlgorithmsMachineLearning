% Clever Algorithms: A Gentle Introduction to Machine Learning

% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2011 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

\documentclass[a4paper, 11pt]{article}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{url}
\usepackage[pdftex,breaklinks=true,colorlinks=true,urlcolor=blue,linkcolor=blue,citecolor=blue,]{hyperref}
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=25mm,bmargin=25mm,lmargin=25mm,rmargin=25mm}

% Dear template user: fill these in
\newcommand{\myreporttitle}{Clever Algorithms}
\newcommand{\myreportsubtitle}{A Gentle Introduction to Machine Learning}
\newcommand{\myreportauthor}{Jason Brownlee}
\newcommand{\myreportemail}{jasonb@CleverAlgorithms.com}
\newcommand{\myreportwebsite}{http://www.CleverAlgorithms.com}
\newcommand{\myreportproject}{The Clever Algorithms Project\\\url{\myreportwebsite}}
\newcommand{\myreportdate}{20110208}
\newcommand{\myreportfulldate}{\today}
\newcommand{\myreportversion}{1}
\newcommand{\myreportlicense}{\copyright\ Copyright 2011 Jason Brownlee. Some Rights Reserved. This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.}

% leave this alone, it's templated baby!
\title{{\myreporttitle}: {\myreportsubtitle}\footnote{\myreportlicense}}
\author{\myreportauthor\\{\myreportemail}\\\small\myreportproject}
\date{\myreportfulldate\\{\small{Technical Report: CA-TR-{\myreportdate}-\myreportversion}}}
\begin{document}
\maketitle

% write a summary sentence for each major section
\section*{Abstract} 
% project
The Clever Algorithms: Machine Learning project intends to describe a large number of methods from the field of Machine Learning in a complete, consistent, and centralized way, such that the descriptions are usable, accessible, and understandable.
% this report
This report provides a gentle introduction into the field of Machine Learning, providing a summary of standard taxonomies of methods, and highlighting important sub-disciplines. 

\begin{description}
	\item[Keywords:] {\small\texttt{Clever Algorithms, Machine Learning, Introduction}}
\end{description} 

% summarise the document breakdown with cross references
\section{Introduction}
\label{sec:introduction}
% project
The Clever Algorithms: Machine Learning project intends to describe a large number of methods from the field of Machine Learning in a complete, consistent, and centralized way, such that the descriptions are usable, accessible, and understandable \cite{Brownlee2011a}.
% this report

% specific sections

\section{Machine Learning}
\label{sec:machine_learning}
% my def
Machine learning is concerned with methods that process empirical data to construct models that contain the salient or underlying probability distribution in the data. Such models may then be applied to address problems or make decisions regarding new data or data unseen by the system.

This section... 

% official definitions
\subsection{Definitions}
\label{subsec:definitions}
This section visits standard definitions and descriptions of the field from some of the classical texts on statistical pattern recognition and machine learning. 

Fukunaga's classic text on statistical pattern recognition describes the field as (page 2--3, \cite{Fukunaga1990})

\begin{quotation}
... pattern recognition, or decision-making in a broader sense, may be considered as a problem of estimating density functions in a high-dimensional space and dividing the space into the regions of categories or classes. Because of this view, mathematical statistics forms the foundation of the subject.
\end{quotation} 

Hastie et~al.\ classic text on statistical learning describes itself ``\emph{This book is about learning from data.}'' (page 1, \cite{Hastie2009}). They describe a typical scenario (pages 1--2, \cite{Hastie2009})

\begin{quotation}
... we have an outcome measurement, usually quantitative ... or categorical ..., that we wish to predict based on a set of features ... We have a training set of data, in which we observe the outcome and feature measurements for a set of objects ... Using this data we build a prediction model, or learner, which will enable us to predict the outcome for new unseen objects. A good learner is one that accurately predicts such an outcome.
\end{quotation} 

Duda et~al.\ classic text on pattern classification suggests (page 16, \cite{Duda2001})

\begin{quotation}
In the broadest sense, any method that incorporates information from training samples in the design of a classifier employs learning. ... Learning refers to some form of algorithm for reducing the error on a set of training data.
\end{quotation}

Mitchell's classic text Machine Learning suggests (page xv, \cite{Mitchell1997})

\begin{quotation}
The field of machine learning is concerned with the question of how to construct computer programs that automatically improve with their experience.
\end{quotation}

Mitchell provides a precise definition which will be used in this project (page 2, \cite{Mitchell1997})

\begin{quotation}
A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measure by P, improves with experience E.
\end{quotation}

Mitchell suggests that the field of Machine Learning seeks to answer the question (\cite{Mitchell2006})

\begin{quotation}
How can we build computer systems that automatically improve with experience, and what are the fundamental laws that govern all learning processes?
\end{quotation}

Some additional excellent resources include Bishop's classical Neural Network text \emph{Neural Networks for Pattern Recognition} \cite{Bishop1995}, and his Machine Learning text \emph{Pattern Recognition and Machine Learning} \cite{Bishop2006}.

% Related fields
\subsection{Related Fields}
\label{subsec:related_fields}
Machine Learning is a cross-disciplinary field that draws upon many related and overarching fields. This section describes some of those fields related to the study of Machine Learning in order to provide context.

% Artificial Intelligence
\subsubsection{Artificial Intelligence}
The field of Artificial Intelligence (AI) coalesced in the 1950s drawing on an understanding of the brain from neuroscience, the new mathematics of information theory, control theory referred to as cybernetics, and the dawn of the digital computer. AI is a cross-disciplinary field of research that is generally concerned with developing and investigating systems that operate or act intelligently. It is considered a discipline in the field of computer science given the strong focus on computation. The study of artificial intelligence is concerned with investigating mechanisms that underlie intelligence and intelligence behavior.

Russell and Norvig provide a perspective that defines artificial intelligence in four categories: 1) systems that think like humans, 2) systems that act like humans, 3) systems that think rationally, 4) systems that act rationally \cite{Russell2009}. In their definition, acting like a human suggests that a system can do some specific things humans can do, this includes fields such as the Turing test, natural language processing, automated reasoning, knowledge representation, machine learning, computer vision, and robotics. Thinking like a human suggests systems that model the cognitive information processing properties of humans, for example a general problem solver and systems that build internal models of their world. Thinking rationally suggests laws of rationalism and structured thought, such as syllogisms and formal logic. Finally, acting rationally suggests systems that do rational things such as expected utility maximization and rational agents. 

Machine Learning is concerned with developing systems that learn from experience -- feature that may be considered a signature of intelligence. As such, machine learning may be considered a sub-field of artificial intelligence.

% Data Mining
\subsubsection{Data Mining}
Broadly, the field of Data Mining may be considered the practical application of statistical and machine learning methods to real-world problem domains. 
Witten and Frank provide a clear definition of the field (page 5, \cite{Witten2000})

\begin{quotation}
Data mining is about solving problems by analyzing data already present in databases. ... Data mining is defined as the process of discovering patterns in data. The process must be automatic or (more usually) semiautomatic. the patterns discovered must be meaningful in that they lead to some advantage usually an economic advantage. The data is invariably present in substantial quantities.
\end{quotation}

The field traditionally uses the phrase `Knowledge Discovery in Databases' (KDD) to describe itself, focusing on the process of applying a suite of machine learning methods to solve a problem \cite{Frawley1992}. An early definition of this process proposed the following elements: Selection, Preprocessing, Transformation, Data Mining, Interpretation and Evaluation, where Data Mining is a single step in the procedure \cite{Fayyad1996a}.
% relation
This suggests Machine Learning methods as the tools that may be used in a KDD process and/or during Data Mining.

% Other
\subsubsection{Other}
This section describes those fields from which Machine Learning draws upon. 

\begin{description}
	\item[Statistics]: The field of statistics is a branch of mathematics concerned the collection, organization, and interpretation of quantitative data. A statistical understanding of data provides a basis for machine learning methods that generalize from quantitative data, commonly characterizing the probabilistic features that underlie the observations.
	\item[Computational Learning Theory]: The field of Computational Learning Theory broadly involves the analysis of machine learning systems. The field provides the theoretical basis for types of learning and their limitations. Machine Learning methods can be developed to investigate a theoretical finding, or be made more efficient through theoretical discoveries. For more information, refer to Kearns and Vazirani's \emph{An introduction to computational learning theory} \cite{Kearns1994}.
	\item[Probability Theory]: The field of probability theory is a branch of mathematics concerned with the likelihood of random events and characterizing the distribution of occurrences. The findings and methods from probability theory provides a basis to the field of statistics. 
	\item[Decision Theory]: The field of decision theory is concerned with rational and optimal decision making in the face of uncertainty. The field is also closely related to game theory and probability theory. The findings from decision theory relate to the application of models in machine learning for the decisions made in discrimination and related tasks.
	\item[Information Theory]	The field of information theory from applied mathematics and computer science is concerned with the quantification of information in data. The field is based on probability theory and statistics, and focuses on the amount or change in information entropy which is the measure of information content associated with a message. Information theory is important in machine learning in compression and generalization that occurs in the preparation of a model from data samples. For more information refer to MacKay's \emph{Information theory, inference, and learning algorithms} \cite{MacKay2003}.
\end{description}

The application of statistical and machine learning methods to specific domains are typically referred to as a standalone field, such as machine vision, speech recognition, machine translation, and information filtering.

% taxonomy by learning
\section{Types of Learning}
\label{sec:learning}
There are different styles of learning that may be adopted by a machine learning method, such as:

\begin{itemize}
	\item \textbf{Supervised Learning}: asdf
	\item \textbf{Unsupervised Learning}: asdf
	\item \textbf{Reinforcement Learning}: asdf
\end{itemize}

% taxonomy by problem
\section{Types of Problems}
\label{sec:problems}
There are many subtly different classes of problem's that may be addressed via machine learning methods, all generally stemming from the abstract problem of Function Approximation. Function Approximation is the problem of finding a function ($f$) that approximates a target function ($g$), where typically the approximated function is selected based on a sample of observations ($x$, also referred to as the training set) taken from the unknown target function.

% ML - todo tear this up some
In Machine Learning, the function approximation formalism is used to describe general problem types commonly referred to as pattern recognition, such as classification, clustering, and curve fitting (called a decision or discrimination function). Such general problem types are described in terms of approximating an unknown Probability Density Function (PDF), which underlies the relationships in the problem space, and is represented in the sample data. This perspective of such problems is commonly referred to as statistical machine learning and/or density estimation \cite{Fukunaga1990, Bishop1995}.

% general process
The general process focuses on 1) the collection and preparation of the observations from the target function, 2) the selection and/or preparation of a model of the target function, and 3) the application and ongoing refinement of the prepared model. 
% optimization
The field of Function Optimization is related to Function Approximation, as many-sub-problems of Function Approximation may be defined as optimization problems. Many of the technique paradigms used for function approximation are differentiated based on the representation and the optimization process used to minimize error or maximize effectiveness on a given approximation problem. 
% problems
The difficulty of Function Approximation problems center around 1) the nature of the unknown relationships between attributes and features, 2) the number (dimensionality) of attributes and features, and 3) general concerns of noise in such relationships and the dynamic availability of samples from the target function.
% other problems
Additional difficulties include the incorporation of prior knowledge (such as imbalance in samples, incomplete information and the variable reliability of data), and problems of invariant features (such as transformation, translation, rotation, scaling, and skewing of features).

The following describes some of the general sub-problems of Function Approximation addressed via Machine Learning methods:

\begin{itemize}
	\item \emph{Feature Selection} where a feature is considered an aggregation of one-or-more attributes, where only those features that have meaning in the context of the target function are necessary to the modeling function \cite{Kudo2000, Guyon2003}.
	\item \emph{Classification} where observations are inherently organized into labeled groups (classes) and a supervised process models an underlying discrimination function to classify unobserved samples.
	\item \emph{Clustering} where observations may be organized into groups based on underlying common features, although the groups are unlabeled requiring a process to model an underlying discrimination function without corrective feedback.
	\item \emph{Curve or Surface Fitting} where a model is prepared that provides a `best-fit' (called a regression) for a set of observations that may be used for interpolation over known observations and extrapolation for observations outside what has been modeled.
	
	\item \emph{Association rule learning}: asdf
	\item \emph{Regression}: asdf

	
\end{itemize}

other
Regularization, Smoothing, Model Selection



% taxonomy by technique
\section{Types of Methods}
\label{sec:methods}
There are many different approaches to devising learning methods, such as:

The list may not be mutually exclusive - expect overlap

\begin{itemize}
	\item \textbf{Decision Tree}: asdf
	\item \textbf{Neural Networks}: asdf
	\item \textbf{Automatic Programming}: asdf
	\item \textbf{Kernel Based Methods}: asdf
	\item \textbf{Bayesian Methods}: asdf
	\item \textbf{Graphical Methods}: frameworks for probabilistic methods (trees could fit in here)
	
	\item \textbf{Instance-Based Learning}: asdf
	\item \textbf{Competitive Learning}: asdf
	
\end{itemize}


% taxonomy of reasoning
\section{Reasoning}
\label{sec:reasoning}

general methods

\begin{itemize}
	\item \textbf{Inductive Learning}: asdf
	\item \textbf{Transductive Learning}: asdf
	\item \textbf{Deductive Learning}: asdf
\end{itemize}


Most methods work through a process of induction. The main types of induction include:
Learning types...

\begin{itemize}
	\item \textbf{Online Learning}: asdf
	\item \textbf{Offline Learning}: asdf
\end{itemize}




% summarise the document message and areas for future consideration
\section{Conclusions}
\label{sec:conclusions}
what have we discussed


what are some areas of further discussion

\begin{itemize}
	\item \textbf{Ensemble Learning}: etc
	\item \textbf{PAC Learning}: etc
	\item \textbf{Optimal Learning}: best learning possible given space and/or time constraints
	\item \textbf{Generalization}:
	\item \textbf{Error Tolerance}:
	\item \textbf{Performance Measures}:
	\item \textbf{Over-fitting}:
	\item \textbf{Methodology}: KDD, CRISP
\end{itemize}



% 
% Contribute
% 
\section{Contribute}
\label{sec:contribute}
% simple
Found a typo in the content or a bug in the source code? 
% advanced 
Are you an expert in this field and know some facts that could improve the algorithm description for all?
% incentive
Do you want to get that warm feeling from contributing to an open source project? 
Do you want to see your name as an acknowledgment in print?

%  ideal
Two pillars of this effort are 1) that the best domain experts are people outside of the project, and 2) that this work is subjected to continuous improvement. 
% advice
Please help to make this work less wrong by emailing the author `\myreportauthor' at \url{\myreportemail} or visit the project website at \url{\myreportwebsite}.

% bibliography
\bibliographystyle{plain}
\bibliography{../bibtex}

\end{document}
% EOF
