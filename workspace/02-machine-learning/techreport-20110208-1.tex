% Clever Algorithms: A Gentle Introduction to Machine Learning

% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2011 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

\documentclass[a4paper, 11pt]{article}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{url}
\usepackage[pdftex,breaklinks=true,colorlinks=true,urlcolor=blue,linkcolor=blue,citecolor=blue,]{hyperref}
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=25mm,bmargin=25mm,lmargin=25mm,rmargin=25mm}

% Dear template user: fill these in
\newcommand{\myreporttitle}{Clever Algorithms}
\newcommand{\myreportsubtitle}{A Gentle Introduction to Machine Learning}
\newcommand{\myreportauthor}{Jason Brownlee}
\newcommand{\myreportemail}{jasonb@CleverAlgorithms.com}
\newcommand{\myreportwebsite}{http://www.CleverAlgorithms.com}
\newcommand{\myreportproject}{The Clever Algorithms Project\\\url{\myreportwebsite}}
\newcommand{\myreportdate}{20110208}
\newcommand{\myreportfulldate}{\today}
\newcommand{\myreportversion}{1}
\newcommand{\myreportlicense}{\copyright\ Copyright 2011 Jason Brownlee. Some Rights Reserved. This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.}

% leave this alone, it's templated baby!
\title{{\myreporttitle}: {\myreportsubtitle}\footnote{\myreportlicense}}
\author{\myreportauthor\\{\myreportemail}\\\small\myreportproject}
\date{\myreportfulldate\\{\small{Technical Report: CA-TR-{\myreportdate}-\myreportversion}}}
\begin{document}
\maketitle

% write a summary sentence for each major section
\section*{Abstract} 
% project
The Clever Algorithms: Machine Learning project intends to describe a large number of methods from the field of Machine Learning in a complete, consistent, and centralized way, such that the descriptions are usable, accessible, and understandable.
% this report
This report provides a gentle introduction into the field of Machine Learning, providing a summary of standard taxonomies of methods, and highlighting important sub-disciplines. 

\begin{description}
	\item[Keywords:] {\small\texttt{Clever Algorithms, Machine Learning, Introduction}}
\end{description} 

% summarise the document breakdown with cross references
\section{Introduction}
\label{sec:introduction}
% project
The Clever Algorithms: Machine Learning project intends to describe a large number of methods from the field of Machine Learning in a complete, consistent, and centralized way, such that the descriptions are usable, accessible, and understandable \cite{Brownlee2011a}.
% this report

% specific sections

\section{Machine Learning}
\label{sec:machine_learning}
% my def
Machine learning is a subfield of Artificial Intelligence and Computer Science concerned with methods that process empirical data to construct models that contain the salient or underlying probability distribution in the data. Such models may then be applied to address problems or make decisions regarding new data or data unseen by the system.

This section... 

% official definitions
\subsection{Definitions}
This section visits standard definitions and descriptions of the field from some of the classical texts on statistical pattern recognition and machine learning. 

Fukunaga's classic text on statistical pattern recognition describes the field as ``... pattern recognition, or decision-making in a broader sense, may be considered as a problem of estimating density functions in a high-dimensional space and dividing the space into the regions of categories or classes. Because of this view, mathematical statistics forms the foundation of the subject.'' (page 2--3) \cite{Fukunaga1990}

Hastie et~al.\ classic text on statistical learning describes itself ``This book is about learning from data.'' (page 1 \cite{Hastie2009}). They describe a typical scenario thus ``we have an outcome measurement, usually quantitative ... or categorical ..., that we wish to predict based on a set of features ... We have a training set of data, in which we observe the outcome and feature measurements for a set of objects ... Using this data we build a prediction model, or learner, which will enable us to predict the outcome for new unseen objects. A good learner is one that accurately predicts such an outcome.'' (pages 1--2 \cite{Hastie2009}).

Mitchell's classic text Machine Learning suggests ``The field of machine learning is concerned with the question of how to construct computer programs that automatically improve with their experience.''  (page xv \cite{Mitchell1997}). Mitchell provides a precise definition as ``A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measure by P, improves with experience E.'' (page 2 \cite{Mitchell1997}).

Mitchell suggests that the field of Machine Learning seeks to answer the question: ``How can we build computer systems that automatically improve with experience, and what are the fundamental laws that govern all learning processes?'' \cite{Mitchell2006}.

Bishop's classical Machine Learning text \cite{Bishop2006}.


% related fields - subsections?
Artificial Intelligence
Computational Learning theory - the analysis of machine learning algorithms.
Data Mining - application of machine learning algorithms to practical domains with the addition of data management concerns (databases)
Statistics  - 
Statistical Pattern Recognition

related theory: probability theory, decision theory, information theory. 

high-level applications
data mining, information retrieval, information filtering, recommendation systems, etc


% taxonomy by learning
\section{Types of Learning}
\label{sec:learning}
There are different styles of learning that may be adopted by a machine learning method, such as:

\begin{itemize}
	\item \textbf{Supervised Learning}: asdf
	\item \textbf{Unsupervised Learning}: asdf
	\item \textbf{Reinforcement Learning}: asdf
\end{itemize}

% taxonomy by problem
\section{Types of Problems}
\label{sec:problems}
There are many subtly different classes of problem's that may be addressed via machine learning methods, all generally stemming from the abstract problem of Function Approximation. Function Approximation is the problem of finding a function ($f$) that approximates a target function ($g$), where typically the approximated function is selected based on a sample of observations ($x$, also referred to as the training set) taken from the unknown target function.

% ML - todo tear this up some
In Machine Learning, the function approximation formalism is used to describe general problem types commonly referred to as pattern recognition, such as classification, clustering, and curve fitting (called a decision or discrimination function). Such general problem types are described in terms of approximating an unknown Probability Density Function (PDF), which underlies the relationships in the problem space, and is represented in the sample data. This perspective of such problems is commonly referred to as statistical machine learning and/or density estimation \cite{Fukunaga1990, Bishop1995}.

% general process
The general process focuses on 1) the collection and preparation of the observations from the target function, 2) the selection and/or preparation of a model of the target function, and 3) the application and ongoing refinement of the prepared model. 
% optimization
The field of Function Optimization is related to Function Approximation, as many-sub-problems of Function Approximation may be defined as optimization problems. Many of the technique paradigms used for function approximation are differentiated based on the representation and the optimization process used to minimize error or maximize effectiveness on a given approximation problem. 
% problems
The difficulty of Function Approximation problems center around 1) the nature of the unknown relationships between attributes and features, 2) the number (dimensionality) of attributes and features, and 3) general concerns of noise in such relationships and the dynamic availability of samples from the target function.
% other problems
Additional difficulties include the incorporation of prior knowledge (such as imbalance in samples, incomplete information and the variable reliability of data), and problems of invariant features (such as transformation, translation, rotation, scaling, and skewing of features).

The following describes some of the general sub-problems of Function Approximation addressed via Machine Learning methods:

\begin{itemize}
	\item \emph{Feature Selection} where a feature is considered an aggregation of one-or-more attributes, where only those features that have meaning in the context of the target function are necessary to the modeling function \cite{Kudo2000, Guyon2003}.
	\item \emph{Classification} where observations are inherently organized into labeled groups (classes) and a supervised process models an underlying discrimination function to classify unobserved samples.
	\item \emph{Clustering} where observations may be organized into groups based on underlying common features, although the groups are unlabeled requiring a process to model an underlying discrimination function without corrective feedback.
	\item \emph{Curve or Surface Fitting} where a model is prepared that provides a `best-fit' (called a regression) for a set of observations that may be used for interpolation over known observations and extrapolation for observations outside what has been modeled.
	
	\item \emph{Association rule learning}: asdf
	\item \emph{Regression}: asdf

	
\end{itemize}

other
Regularization, Smoothing, Model Selection



% taxonomy by technique
\section{Types of Methods}
\label{sec:methods}
There are many different approaches to devising learning methods, such as:

The list may not be mutually exclusive - expect overlap

\begin{itemize}
	\item \textbf{Decision Tree}: asdf
	\item \textbf{Neural Networks}: asdf
	\item \textbf{Automatic Programming}: asdf
	\item \textbf{Kernel Based Methods}: asdf
	\item \textbf{Bayesian Methods}: asdf
	\item \textbf{Graphical Methods}: frameworks for probabilistic methods (trees could fit in here)
	
	\item \textbf{Instance-Based Learning}: asdf
	\item \textbf{Competitive Learning}: asdf
	
\end{itemize}


% taxonomy of reasoning
\section{Reasoning}
\label{sec:reasoning}

general methods

\begin{itemize}
	\item \textbf{Inductive Learning}: asdf
	\item \textbf{Transductive Learning}: asdf
	\item \textbf{Deductive Learning}: asdf
\end{itemize}


Most methods work through a process of induction. The main types of induction include:
Learning types...

\begin{itemize}
	\item \textbf{Online Learning}: asdf
	\item \textbf{Offline Learning}: asdf
\end{itemize}




% summarise the document message and areas for future consideration
\section{Conclusions}
\label{sec:conclusions}
what have we discussed


what are some areas of further discussion

\begin{itemize}
	\item \textbf{Ensemble Learning}: etc
	\item \textbf{PAC Learning}: etc
	\item \textbf{Optimal Learning}: best learning possible given space and/or time constraints
	\item \textbf{Generalization}:
	\item \textbf{Error Tolerance}:
	\item \textbf{Performance Measures}:
	\item \textbf{Over-fitting}:
	\item \textbf{Methodology}: KDD, CRISP
\end{itemize}



% 
% Contribute
% 
\section{Contribute}
\label{sec:contribute}
% simple
Found a typo in the content or a bug in the source code? 
% advanced 
Are you an expert in this field and know some facts that could improve the algorithm description for all?
% incentive
Do you want to get that warm feeling from contributing to an open source project? 
Do you want to see your name as an acknowledgment in print?

%  ideal
Two pillars of this effort are 1) that the best domain experts are people outside of the project, and 2) that this work is subjected to continuous improvement. 
% advice
Please help to make this work less wrong by emailing the author `\myreportauthor' at \url{\myreportemail} or visit the project website at \url{\myreportwebsite}.

% bibliography
\bibliographystyle{plain}
\bibliography{../bibtex}

\end{document}
% EOF
