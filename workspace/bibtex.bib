% This file was created with JabRef 2.7.2.
% Encoding: UTF-8

@ARTICLE{Breiman2001,
  author = {Leo Breiman},
  title = {Random Forests},
  journal = {Machine Learning},
  year = {2001},
  volume = {45},
  pages = {5--32},
  number = {1},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1023/A:1010933404324}
}

@ARTICLE{Breiman1996,
  author = {Breiman, Leo},
  title = {Bagging predictors},
  journal = {Mach. Learn.},
  year = {1996},
  volume = {24},
  pages = {123--140},
  month = {August},
  acmid = {231989},
  address = {Hingham, MA, USA},
  doi = {10.1023/A:1018054314350},
  issn = {0885-6125},
  issue = {2},
  keywords = {aggregation, averaging, bootstrap, combining},
  numpages = {18},
  publisher = {Kluwer Academic Publishers},
  url = {http://dl.acm.org/citation.cfm?id=231986.231989}
}

@TECHREPORT{Breiman2003,
  author = {Leo Breiman and Adele Cutler},
  title = {Manual: Setting up, using, and understanding Random Forests v4.0},
  institution = {University of California, Berkeley},
  year = {2003},
  owner = {brownlee},
  timestamp = {2011.12.09}
}

@BOOK{Breiman1984,
  title = {Classification and regression trees},
  publisher = {Chapman and Hall},
  year = {1984},
  author = {Breiman, Leo and Friedman, J. H. and Olshen, R. A. and Stone, C.
	J.},
  owner = {brownlee},
  timestamp = {2011.11.23}
}

@ARTICLE{Broyden1970,
  author = {Broyden, C. G.},
  title = {{The Convergence of a Class of Double-rank Minimization Algorithms
	1. General Considerations}},
  journal = {IMA J Appl Math},
  year = {1970},
  volume = {6},
  pages = {76--90},
  number = {1},
  month = mar,
  abstract = {{This paper presents a more detailed analysis of a class of minimization
	algorithms, which includes as a special case the DFP (Davidon-Fletcher-Powell)
	method, than has previously appeared. Only quadratic functions are
	considered but particular attention is paid to the magnitude of successive
	errors and their dependence upon the initial matrix. On the basis
	of this a possible explanation of some of the observed characteristics
	of the class is tentatively suggested. 10.1093/imamat/6.1.76}},
  citeulike-article-id = {2945939},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/imamat/6.1.76},
  citeulike-linkout-1 = {http://imamat.oxfordjournals.org/cgi/content/abstract/6/1/76},
  day = {1},
  doi = {10.1093/imamat/6.1.76},
  posted-at = {2008-06-30 22:48:42},
  priority = {1},
  url = {http://dx.doi.org/10.1093/imamat/6.1.76}
}

@INPROCEEDINGS{Dietterich2000,
  author = {Dietterich, Thomas G.},
  title = {Ensemble Methods in Machine Learning},
  booktitle = {Proceedings of the First International Workshop on Multiple Classifier
	Systems},
  year = {2000},
  series = {MCS '00},
  pages = {1--15},
  address = {London, UK},
  publisher = {Springer-Verlag},
  acmid = {743935},
  isbn = {3-540-67704-6},
  numpages = {15},
  url = {http://dl.acm.org/citation.cfm?id=648054.743935}
}

@TECHREPORT{Dietterich1995,
  author = {Dietterich, T. G., and Kong, E. B.},
  title = {Machine Learning Bias, Statistical Bias, and Statistical Variance
	of Decision Tree Algorithms},
  institution = {Department of Computer Science, Oregon State University},
  year = {1995},
  address = {Corvallis, Oregon},
  owner = {brownlee},
  timestamp = {2011.11.24}
}

@ARTICLE{Fletcher1970,
  author = {Fletcher, R.},
  title = {{A new approach to variable metric algorithms}},
  journal = {The Computer Journal},
  year = {1970},
  volume = {13},
  pages = {317--322},
  number = {3},
  month = mar,
  abstract = {{An approach to variable metric algorithms has been investigated in
	which the linear search sub-problem no longer becomes necessary.
	The property of quadratic termination has been replaced by one of
	monotonic convergence of the eigenvalues of the approximating matrix
	to the inverse hessian. A convex class of updating formulae which
	possess this property has been established, and a strategy has been
	indicated for choosing a member of the class so as to keep the approximation
	away from both singularity and unboundedness. A FORTRAN program has
	been tested extensively with encouraging results. 10.1093/comjnl/13.3.317}},
  citeulike-article-id = {7133130},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/comjnl/13.3.317},
  day = {1},
  doi = {10.1093/comjnl/13.3.317},
  keywords = {bfgs},
  posted-at = {2010-05-07 01:16:26},
  priority = {2},
  url = {http://dx.doi.org/10.1093/comjnl/13.3.317}
}

@ARTICLE{Fletcher1964,
  author = {Fletcher, R and Reeves, C M},
  title = {Function minimization by conjugate gradients},
  journal = {The Computer Journal},
  year = {1964},
  volume = {7},
  pages = {149--154},
  number = {2},
  publisher = {Br Computer Soc},
  url = {http://comjnl.oupjournals.org/cgi/doi/10.1093/comjnl/7.2.149}
}

@BOOK{Foulkes2009,
  title = {Applied Statistical Genetics with {R}: For Population-Based Association
	Studies},
  publisher = {Springer},
  year = {2009},
  author = {Andrea S. Foulkes},
  series = {Use R},
  abstract = {In this introductory graduate level text, Dr.~Foulkes elucidates core
	concepts that undergird the wide range of analytic techniques and
	software tools for the analysis of data derived from population-based
	genetic investigations. Applied Statistical Genetics with R offers
	a clear and cogent presentation of several fundamental statistical
	approaches that researchers from multiple disciplines, including
	medicine, public health, epidemiology, statistics and computer science,
	will find useful in exploring this emerging field.},
  isbn = {978-0-387-89553-6},
  orderinfo = {springer.txt},
  publisherurl = {http://www.springeronline.com/978-0-387-89553-6}
}

@ARTICLE{Freund1997,
  author = {Yoav Freund and Robert E. Schapire},
  title = {A Decision--Theoretic Generalization of On--Line Learning and an
	Application to Boosting},
  journal = {Journal of Computer and System Sciences},
  year = {1997},
  volume = {55},
  pages = {119--139},
  number = {1},
  doi = {10.1006/jcss.1997.1504},
  issn = {0022-0000},
  url = {http://www.sciencedirect.com/science/article/pii/S002200009791504X}
}

@ARTICLE{Friedman2001,
  author = {Friedman, Jerome H.},
  title = {{Greedy function approximation: A gradient boosting machine}},
  journal = {Annals of Statistics},
  year = {2001},
  volume = {29},
  pages = {1189--1232},
  abstract = {{Function approximation is viewed from the perspective of numerical
	optimization in function space, rather than parameter space. A connection
	is made between stagewise additive expansions and steepest--descent
	minimization. A general gradient--descent \&amp;quot;boosting\&amp;quot;
	paradigm is developed for additive expansions based on any fitting
	criterion. Specific algorithms are presented for least--squares,
	least--absolute--deviation, and Huber--M loss functions for regression,
	and multi--class logistic likelihood for classification. Special
	enhancements are derived for the particular case where the individual
	additive components are decision trees, and tools for interpreting
	such \&amp;quot;TreeBoost \&amp;quot; models are presented. Gradient
	boosting of decision trees produces competitive, highly robust, interpretable
	procedures for regression and classification, especially appropriate
	for mining less than clean data. Connections between this approach
	and the boosting methods of Freund and Shapire 1996, and Friedman,
	Hastie, and Tibshirani 1998 are discussed.}},
  citeulike-article-id = {3154111},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.869},
  posted-at = {2008-08-25 20:40:24},
  priority = {2},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.869}
}

@ARTICLE{Friedman1991,
  author = {Friedman, Jerome H.},
  title = {{Multivariate Adaptive Regression Splines}},
  journal = {The Annals of Statistics},
  year = {1991},
  volume = {19},
  pages = {1--67},
  number = {1},
  abstract = {{A new method is presented for flexible regression modeling of high
	dimensional data. The model takes the form of an expansion in product
	spline basis functions, where the number of basis functions as well
	as the parameters associated with each one (product degree and knot
	locations) are automatically determined by the data. This procedure
	is motivated by the recursive partitioning approach to regression
	and shares its attractive properties. Unlike recursive partitioning,
	however, this method produces continuous models with continuous derivatives.
	It has more power and flexibility to model relationships that are
	nearly additive or involve interactions in at most a few variables.
	In addition, the model can be represented in a form that separately
	identifies the additive contributions and those associated with the
	different multivariable interactions.}},
  citeulike-article-id = {4371368},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2241837},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2241837},
  doi = {10.2307/2241837},
  issn = {00905364},
  keywords = {nonparametric\_regression, statistics},
  posted-at = {2009-04-21 09:08:01},
  priority = {2},
  publisher = {Institute of Mathematical Statistics},
  url = {http://dx.doi.org/10.2307/2241837}
}

@ARTICLE{Goldfarb1970,
  author = {Goldfarb, D.},
  title = {{A Family of Variable Metric Updates Derived by Variational Means}},
  journal = {Mathematics of Computation},
  year = {1970},
  volume = {24},
  pages = {23--26},
  citeulike-article-id = {8491642},
  posted-at = {2010-12-29 19:26:05},
  priority = {2}
}

@ARTICLE{Hocking1976,
  author = {Hocking, R R},
  title = {The Analysis and Selection of Variables in Linear Regression},
  journal = {Biometrics},
  year = {1976},
  volume = {32},
  pages = {1--49},
  number = {1},
  url = {http://www.jstor.org/stable/2529336}
}

@BOOK{Hosmer2000,
  title = {Applied logistic regression},
  publisher = {Wiley},
  year = {2000},
  author = {Hosmer, D.W. and Lemeshow, S.},
  series = {Wiley series in probability and statistics: Texts and references
	section},
  isbn = {9780471356325},
  lccn = {00036843},
  url = {http://books.google.com.au/books?id=DEtar8K5ASsC}
}

@ARTICLE{Karatzoglou2006,
  author = {Alexandros Karatzoglou and David Meyer and Kurt Hornik},
  title = {Support Vector Machines in R},
  journal = {Journal of Statistical Software},
  year = {2006},
  volume = {15},
  pages = {1--28},
  number = {9},
  month = {4},
  accepted = {2006-04-06},
  bibdate = {2006-04-06},
  coden = {JSSOBK},
  day = {6},
  issn = {1548-7660},
  submitted = {2005-10-24},
  url = {http://www.jstatsoft.org/v15/i09}
}

@BOOK{Kleinbaum2010,
  title = {Logistic Regression: A Self-Learning Text},
  publisher = {Springer},
  year = {2010},
  author = {Kleinbaum, D.G. and Klein, M. and Pryor, E.R.},
  series = {Statistics for Biology and Health},
  isbn = {9781441917416},
  lccn = {2009943538},
  url = {http://books.google.com.au/books?id=J7E0JQweHkoC}
}

@TECHREPORT{Komarek2005,
  author = {Paul Komarek and Andrew Moore},
  title = {Making Logistic Regression A Core Data Mining Tool: A Practical Investigation
	of Accuracy, Speed, and Simplicity},
  institution = {Robotics Institute, Carnegie Mellon University},
  year = {2005},
  number = {CMU-RI-TR-05-27},
  address = {Pittsburgh, PA},
  month = {May},
  howpublished = {technical report},
  pages = {13},
  volume = {TR-05-27}
}

@ARTICLE{Lagarias1998,
  author = {Lagarias, Jeffrey C. and Reeds, James A. and Wright, Margaret H.
	and Wright, Paul E.},
  title = {Convergence Properties of the Nelder--Mead Simplex Method in Low
	Dimensions},
  journal = {SIAM J. on Optimization},
  year = {1998},
  volume = {9},
  pages = {112--147},
  month = {May},
  acmid = {589108},
  address = {Philadelphia, PA, USA},
  doi = {http://dx.doi.org/10.1137/S1052623496303470},
  issn = {1052-6234},
  issue = {1},
  keywords = {Nelder--Mead simplex methods, direct search methods, nonderivative
	optimization},
  numpages = {36},
  publisher = {Society for Industrial and Applied Mathematics},
  url = {http://dx.doi.org/10.1137/S1052623496303470}
}

@ARTICLE{Liaw2002,
  author = {Andy Liaw and Matthew Wiener},
  title = {Classiﬁcation and Regression by randomForest},
  journal = {R News},
  year = {2002},
  volume = {2},
  pages = {18--22},
  number = {3},
  month = {December},
  owner = {brownlee},
  timestamp = {2011.12.09}
}

@TECHREPORT{Meyer2011,
  author = {David Meyer},
  title = {Support Vector Machines: The Interface to libsvm in package e1071},
  institution = {Technische Universitat Wien, Austria},
  year = {2011},
  owner = {brownlee},
  timestamp = {2011.12.06}
}

@ARTICLE{Mills2010,
  author = {Mills, Josh},
  title = {Estimation of Statistical Models in R},
  journal = {Syntax},
  year = {2010},
  pages = {1--145}
}

@ARTICLE{Mundry2009,
  author = {Mundry, Roger and Nunn, Charles L},
  title = {Stepwise model fitting and statistical inference: turning noise into
	signal pollution.},
  journal = {The American naturalist},
  year = {2009},
  volume = {173},
  pages = {119--23},
  number = {1},
  publisher = {UChicago Press},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/19049440}
}

@ARTICLE{Nelder1965,
  author = {Nelder, J. A. and Mead, R.},
  title = {{A Simplex Method for Function Minimization}},
  journal = {The Computer Journal},
  year = {1965},
  volume = {7},
  pages = {308--313},
  number = {4},
  month = jan,
  abstract = {{A method is described for the minimization of a function of n variables,
	which depends on the comparison of function values at the (n + 1)
	vertices of a general simplex, followed by the replacement of the
	vertex with the highest value by another point. The simplex adapts
	itself to the local landscape, and contracts on to the final minimum.
	The method is shown to be effective and computationally compact.
	A procedure is given for the estimation of the Hessian matrix in
	the neighbourhood of the minimum, needed in statistical estimation
	problems.}},
  citeulike-article-id = {3009487},
  citeulike-linkout-0 = {http://dx.doi.org/10.1093/comjnl/7.4.308},
  citeulike-linkout-1 = {http://comjnl.oxfordjournals.org/content/7/4/308.abstract},
  citeulike-linkout-2 = {http://comjnl.oxfordjournals.org/content/7/4/308.full.pdf},
  citeulike-linkout-3 = {http://comjnl.oxfordjournals.org/cgi/content/abstract/7/4/308},
  day = {1},
  doi = {10.1093/comjnl/7.4.308},
  posted-at = {2008-09-15 16:23:09},
  priority = {2},
  url = {http://dx.doi.org/10.1093/comjnl/7.4.308}
}

@BOOK{Pampel2000,
  title = {Logistic regression: a primer},
  publisher = {Sage Publications},
  year = {2000},
  author = {Pampel, F.C.},
  series = {Sage university papers series: Quantitative applications in the social
	sciences},
  isbn = {9780761920106},
  lccn = {00008060},
  url = {http://books.google.com.au/books?id=lfzSqxFceq0C}
}

@ARTICLE{Peduzzi1996,
  author = {Peduzzi, Peter and Concato, John and Kemper, Elizabeth and Holford,
	Theodore R. and Feinstein, Alvan R.},
  title = {{A simulation study of the number of events per variable in logistic
	regression analysis}},
  journal = {Journal of Clinical Epidemiology},
  year = {1996},
  volume = {49},
  pages = {1373--1379},
  number = {12},
  month = dec,
  abstract = {{We performed a Monte Carlo study to evaluate the effect of the number
	of events per variable (EPV) analyzed in logistic regression analysis.
	The simulations were based on data from a cardiac trial of 673 patients
	in which 252 deaths occurred and seven variables were cogent predictors
	of mortality; the number of events per predictive variable was ()
	for the full sample. For the simulations, at values of EPV = 2, 5,
	10, 15, 20, and 25, we randomly generated 500 samples of the 673
	patients, chosen with replacement, according to a logistic model
	derived from the full sample. Simulation results for the regression
	coefficients for each variable in each group of 500 samples were
	compared for bias, precision, and significance testing against the
	results of the model fitted to the original sample. For EPV values
	of 10 or greater, no major problems occurred. For EPV values less
	than 10, however, the regression coefficients were biased in both
	positive and negative directions; the large sample variance estimates
	from the logistic model both overestimated and underestimated the
	sample variance of the regression coeffi-cients; the 90\% confidence
	limits about the estimated values did not have proper coverage; the
	Wald statistic was conservative under the null hypothesis; and paradoxical
	associations (significance in the wrong direction) were increased.
	Although other factors (such as the total number of events, or sample
	size) may influence the validity of the logistic model, our findings
	indicate that low EPV can lead to major problems.}},
  citeulike-article-id = {2305744},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/S0895-4356(96)00236-3},
  citeulike-linkout-1 = {http://linkinghub.elsevier.com/retrieve/pii/S0895-4356(96)00236-3},
  citeulike-linkout-2 = {http://www.sciencedirect.com/science/article/B6T84-3W30X5C-8/2/f1b4e237713e6d272e85c988556f1d4e},
  doi = {10.1016/S0895-4356(96)00236-3},
  issn = {08954356},
  keywords = {logistic, model\_selection, power},
  posted-at = {2011-03-24 19:17:32},
  priority = {2},
  url = {http://dx.doi.org/10.1016/S0895-4356(96)00236-3}
}

@BOOK{Quinlan1993,
  title = {C4.5: Programs for Machine Learning},
  publisher = {Morgan Kaufmann},
  year = {1993},
  author = {J. R. Quinlan},
  owner = {brownlee},
  timestamp = {2011.11.23}
}

@MANUAL{RDevelopmentCoreTeam2011,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Development Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2011},
  note = {{ISBN} 3-900051-07-0},
  url = {http://www.R-project.org}
}

@ARTICLE{Shanno1970,
  author = {Shanno, D. F.},
  title = {{Conditioning of Quasi-Newton Methods for Function Minimization}},
  journal = {Mathematics of Computation},
  year = {1970},
  volume = {24},
  pages = {647--656},
  number = {111},
  month = jul,
  abstract = {{Quasi-Newton methods accelerate the steepest-descent technique for
	function minimization by using computational history to generate
	a sequence of approximations to the inverse of the Hessian matrix.
	This paper presents a class of approximating matrices as a function
	of a scalar parameter. The problem of optimal conditioning of these
	matrices under an appropriate norm as a function of the scalar parameter
	is investigated. A set of computational results verifies the superiority
	of the new methods arising from conditioning considerations to known
	methods.}},
  citeulike-article-id = {4267226},
  citeulike-linkout-0 = {http://dx.doi.org/10.2307/2004840},
  citeulike-linkout-1 = {http://www.jstor.org/stable/2004840},
  doi = {10.2307/2004840},
  issn = {00255718},
  keywords = {qbo\_nlpca2c},
  posted-at = {2009-04-03 02:28:35},
  priority = {2},
  publisher = {American Mathematical Society},
  url = {http://dx.doi.org/10.2307/2004840}
}

@TECHREPORT{Shewchuk1994,
  author = {Shewchuk, Jonathan R.},
  title = {{An Introduction to the Conjugate Gradient Method Without the Agonizing
	Pain}},
  year = {1994},
  address = {Pittsburgh, PA, USA},
  citeulike-article-id = {264342},
  citeulike-linkout-0 = {http://www.cs.cmu.edu/\~{}quake-papers/painless-conjugate-gradient.pdf},
  citeulike-linkout-1 = {http://portal.acm.org/citation.cfm?id=865018},
  keywords = {optimization},
  posted-at = {2007-10-09 14:30:47},
  priority = {2},
  publisher = {Carnegie Mellon University},
  url = {http://www.cs.cmu.edu/\~{}quake-papers/painless-conjugate-gradient.pdf}
}

@ARTICLE{Spendley1962,
  author = {Spendley, W. and Hext, G. R. and Himsworth, F. R.},
  title = {{Sequential Application of Simplex Designs in Optimization and Evolutionary
	Operation}},
  journal = {Technometrics},
  year = {1962},
  volume = {4},
  pages = {441--461},
  citeulike-article-id = {2846890},
  keywords = {dfo, optimisation, simplex},
  posted-at = {2008-05-30 10:37:13},
  priority = {2}
}

@ARTICLE{Strobl2009,
  author = {Carolin Strobl and Torsten Hothorn and Achim Zeileis},
  title = {Party on! A New, Conditional Variable-Importance Measure for Random
	Forests Available in the party Package},
  journal = {The R Journal},
  year = {2009},
  volume = {1/2},
  pages = {14--17},
  owner = {brownlee},
  timestamp = {2011.12.09}
}

@ARTICLE{Tibshirani1996,
  author = {Tibshirani, Robert},
  title = {{Regression shrinkage and selection via the lasso}},
  journal = {J. Roy. Statist. Soc. Ser. B},
  year = {1996},
  volume = {58},
  pages = {267--288},
  number = {1},
  abstract = {{We propose a new method for estimation in linear models. The \&quot;lasso\&quot;
	minimizes the residual sum of squares subject to the sum of the absolute
	value of the coefficients being less than a constant. Because of
	the nature of this constraint it tends to produce some coefficients
	that are exactly zero and hence gives interpretable models. Our simulation
	studies suggest that the lasso enjoys some of the favourable properties
	of both subset selection and ridge regression. It produces interpretable
	models like subset selection and exhibits the stability of ridge
	regression. There is also an interesting relationship with recent
	work in adaptive function estimation by Donoho and Johnstone. The
	lasso idea is quite general and can be applied in a variety of statistical
	models: extensions to generalized regression models}},
  citeulike-article-id = {416068},
  citeulike-linkout-0 = {http://www.ams.org/mathscinet-getitem?mr=1379242},
  keywords = {feature-selection, lasso, regression, ridge-regression, shrinkage},
  mrnumber = {MR1379242},
  posted-at = {2008-11-13 17:23:13},
  priority = {3},
  url = {http://www.ams.org/mathscinet-getitem?mr=1379242}
}

@TECHREPORT{Tibshirani1996a,
  author = {Tibshirani, R.},
  title = {{Bias, Variance, and Prediction Error for Classification Rules}},
  institution = {Department of Statistics, University of Toronto},
  year = {1996},
  citeulike-article-id = {340511},
  keywords = {biasvariance, diplomarbeit},
  posted-at = {2005-10-04 12:14:09},
  priority = {0}
}

@BOOK{Torgo2009,
  title = {Data Mining with R},
  year = {2009},
  author = {Torgo, Luís},
  added-at = {2009-01-22T17:32:38.000+0100},
  biburl = {http://www.bibsonomy.org/bibtex/23a150daaf62344685dd63088a995a9c5/tmalsburg},
  description = {The main goal of this book is to introduce the reader to the use of
	R as a tool for performing data mining.},
  interhash = {427fb0ce49a4101586b5855df045fce5},
  intrahash = {3a150daaf62344685dd63088a995a9c5},
  keywords = {book datamining gnu-r introduction neuralnetworks statistics trading},
  timestamp = {2009-01-22T17:32:38.000+0100}
}

@BOOK{Walters1991,
  title = {Sequential simplex optimization: a technique for improving quality
	and productivity in research, development, and manufacturing},
  publisher = {CRC Press},
  year = {1991},
  author = {Walters, F.H.},
  series = {Chemometrics series},
  isbn = {9780849358944},
  lccn = {91014187},
  url = {http://books.google.com.au/books?id=hpxTAAAAMAAJ}
}

@ARTICLE{Whittingham2006,
  author = {Whittingham, Mark J. and Stephens, Philip A. and Bradbury, Richard
	B. and Freckleton, Robert P.},
  title = {{Why do we still use stepwise modelling in ecology and behaviour?}},
  journal = {Journal of Animal Ecology},
  year = {2006},
  volume = {75},
  pages = {1182--1189},
  number = {5},
  month = sep,
  abstract = {{Summary * 1The biases and shortcomings of stepwise multiple regression
	are well established within the statistical literature. However,
	an examination of papers published in 2004 by three leading ecological
	and behavioural journals suggested that the use of this technique
	remains widespread: of 65 papers in which a multiple regression approach
	was used, 57\% of studies used a stepwise procedure. * 2The principal
	drawbacks of stepwise multiple regression include bias in parameter
	estimation, inconsistencies among model selection algorithms, an
	inherent (but often overlooked) problem of multiple hypothesis testing,
	and an inappropriate focus or reliance on a single best model. We
	discuss each of these issues with examples. * 3We use a worked example
	of data on yellowhammer distribution collected over 4 years to highlight
	the pitfalls of stepwise regression. We show that stepwise regression
	allows models containing significant predictors to be obtained from
	each year's data. In spite of the significance of the selected models,
	they vary substantially between years and suggest patterns that are
	at odds with those determined by analysing the full, 4-year data
	set. * 4An information theoretic (IT) analysis of the yellowhammer
	data set illustrates why the varying outcomes of stepwise analyses
	arise. In particular, the IT approach identifies large numbers of
	competing models that could describe the data equally well, showing
	that no one model should be relied upon for inference.}},
  address = {Department of Mathematics, University of Bristol, University Walk,
	Bristol, BS8 1TW, UK; ; Royal Society for the Protection of Birds,
	The Lodge, Sandy, Bedfordshire, SG19 2DL, UK; and ; Department of
	Animal and Plant Sciences, University of Sheffield, Sheffield S10
	2TN, UK},
  citeulike-article-id = {786972},
  citeulike-linkout-0 = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.1365-2656.2006.01141.x},
  citeulike-linkout-1 = {http://dx.doi.org/10.1111/j.1365-2656.2006.01141.x},
  citeulike-linkout-2 = {http://www.ingentaconnect.com/content/bsc/janim/2006/00000075/00000005/art00016},
  citeulike-linkout-3 = {http://view.ncbi.nlm.nih.gov/pubmed/16922854},
  citeulike-linkout-4 = {http://www.hubmed.org/display.cgi?uids=16922854},
  citeulike-linkout-5 = {http://www3.interscience.wiley.com/cgi-bin/abstract/118727122/ABSTRACT},
  doi = {10.1111/j.1365-2656.2006.01141.x},
  issn = {0021-8790},
  keywords = {plant\_ecology, statistics},
  pmid = {16922854},
  posted-at = {2009-03-23 10:12:31},
  priority = {2},
  publisher = {Blackwell Publishing Ltd},
  url = {http://dx.doi.org/10.1111/j.1365-2656.2006.01141.x}
}

@ARTICLE{Zhu1997,
  author = {Zhu, Ciyou and Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge},
  title = {Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained
	optimization},
  journal = {ACM Trans. Math. Softw.},
  year = {1997},
  volume = {23},
  pages = {550--560},
  month = {December},
  acmid = {279236},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/279232.279236},
  issn = {0098-3500},
  issue = {4},
  issue_date = {Dec. 1997},
  keywords = {large-scale optimization, limited-memory method, nonlinear optimization,
	variable metric method},
  numpages = {11},
  publisher = {ACM},
  url = {http://doi.acm.org/10.1145/279232.279236}
}

