% This file was created with JabRef 2.7.2.
% Encoding: UTF-8

@ARTICLE{Breiman2001,
  author = {Leo Breiman},
  title = {Random Forests},
  journal = {Machine Learning},
  year = {2001},
  volume = {45},
  pages = {5--32},
  number = {1},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1023/A:1010933404324}
}

@ARTICLE{Breiman1996,
  author = {Breiman, Leo},
  title = {Bagging predictors},
  journal = {Mach. Learn.},
  year = {1996},
  volume = {24},
  pages = {123--140},
  month = {August},
  acmid = {231989},
  address = {Hingham, MA, USA},
  doi = {10.1023/A:1018054314350},
  issn = {0885-6125},
  issue = {2},
  keywords = {aggregation, averaging, bootstrap, combining},
  numpages = {18},
  publisher = {Kluwer Academic Publishers},
  url = {http://dl.acm.org/citation.cfm?id=231986.231989}
}

@BOOK{Breiman1984,
  title = {Classification and regression trees},
  publisher = {Chapman and Hall},
  year = {1984},
  author = {Breiman, Leo and Friedman, J. H. and Olshen, R. A. and Stone, C.
	J.},
  owner = {brownlee},
  timestamp = {2011.11.23}
}

@INPROCEEDINGS{Dietterich2000,
  author = {Dietterich, Thomas G.},
  title = {Ensemble Methods in Machine Learning},
  booktitle = {Proceedings of the First International Workshop on Multiple Classifier
	Systems},
  year = {2000},
  series = {MCS '00},
  pages = {1--15},
  address = {London, UK},
  publisher = {Springer-Verlag},
  acmid = {743935},
  isbn = {3-540-67704-6},
  numpages = {15},
  url = {http://dl.acm.org/citation.cfm?id=648054.743935}
}

@ARTICLE{Freund1997,
  author = {Yoav Freund and Robert E. Schapire},
  title = {A Decision--Theoretic Generalization of On--Line Learning and an
	Application to Boosting},
  journal = {Journal of Computer and System Sciences},
  year = {1997},
  volume = {55},
  pages = {119--139},
  number = {1},
  doi = {10.1006/jcss.1997.1504},
  issn = {0022-0000},
  url = {http://www.sciencedirect.com/science/article/pii/S002200009791504X}
}

@ARTICLE{Friedman2001,
  author = {Friedman, Jerome H.},
  title = {{Greedy function approximation: A gradient boosting machine}},
  journal = {Annals of Statistics},
  year = {2001},
  volume = {29},
  pages = {1189--1232},
  abstract = {{Function approximation is viewed from the perspective of numerical
	optimization in function space, rather than parameter space. A connection
	is made between stagewise additive expansions and steepest--descent
	minimization. A general gradient--descent \&amp;quot;boosting\&amp;quot;
	paradigm is developed for additive expansions based on any fitting
	criterion. Specific algorithms are presented for least--squares,
	least--absolute--deviation, and Huber--M loss functions for regression,
	and multi--class logistic likelihood for classification. Special
	enhancements are derived for the particular case where the individual
	additive components are decision trees, and tools for interpreting
	such \&amp;quot;TreeBoost \&amp;quot; models are presented. Gradient
	boosting of decision trees produces competitive, highly robust, interpretable
	procedures for regression and classification, especially appropriate
	for mining less than clean data. Connections between this approach
	and the boosting methods of Freund and Shapire 1996, and Friedman,
	Hastie, and Tibshirani 1998 are discussed.}},
  citeulike-article-id = {3154111},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.869},
  posted-at = {2008-08-25 20:40:24},
  priority = {2},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.869}
}

@BOOK{Quinlan1993,
  title = {C4.5: Programs for Machine Learning},
  publisher = {Morgan Kaufmann},
  year = {1993},
  author = {J. R. Quinlan},
  owner = {brownlee},
  timestamp = {2011.11.23}
}

@ARTICLE{Tibshirani1996,
  author = {Tibshirani, Robert},
  title = {{Regression shrinkage and selection via the lasso}},
  journal = {J. Roy. Statist. Soc. Ser. B},
  year = {1996},
  volume = {58},
  pages = {267--288},
  number = {1},
  abstract = {{We propose a new method for estimation in linear models. The \&quot;lasso\&quot;
	minimizes the residual sum of squares subject to the sum of the absolute
	value of the coefficients being less than a constant. Because of
	the nature of this constraint it tends to produce some coefficients
	that are exactly zero and hence gives interpretable models. Our simulation
	studies suggest that the lasso enjoys some of the favourable properties
	of both subset selection and ridge regression. It produces interpretable
	models like subset selection and exhibits the stability of ridge
	regression. There is also an interesting relationship with recent
	work in adaptive function estimation by Donoho and Johnstone. The
	lasso idea is quite general and can be applied in a variety of statistical
	models: extensions to generalized regression models}},
  citeulike-article-id = {416068},
  citeulike-linkout-0 = {http://www.ams.org/mathscinet-getitem?mr=1379242},
  keywords = {feature-selection, lasso, regression, ridge-regression, shrinkage},
  mrnumber = {MR1379242},
  posted-at = {2008-11-13 17:23:13},
  priority = {3},
  url = {http://www.ams.org/mathscinet-getitem?mr=1379242}
}

